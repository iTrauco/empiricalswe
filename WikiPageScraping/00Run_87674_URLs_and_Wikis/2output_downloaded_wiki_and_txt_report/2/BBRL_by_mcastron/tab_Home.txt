[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p>BBRL (<strong>B</strong>enchmarking tools for <strong>B</strong>ayesian <strong>R</strong>einforcement <strong>L</strong>earning) is a C++ open-source library for Bayesian reinforcement learning (discrete state/action spaces). We decided to develop these tools by gathering as many agents and experiments as possible in order to provide a unified framework for Bayesian reinforcement learning.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#problem-statement" id="user-content-problem-statement"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Problem Statement</h1>
<p>Reinforcement Learning is a field of Machine Learning, which aims to learn how to behave in different circumstances based on previous experiences. We consider an agent interacting with an environment. At each time-step, the agent is in a given state and has to perform an action. In return, the environment sends a reward and moves the agent to another state. The goal of the agent is to optimise the rewards collected by taking good decisions, based on its current state and what has happened during the previous interactions.</p>
<p>The environment is generally represented by a Markov Decision Process (MDP), which is defined as follows:</p>
<ul>
<li>
<strong>The state space</strong>, which is the set of all possible states in which an agent can be.</li>
<li>
<strong>The action space</strong>, which is the set of all possible actions that an agent can perform.</li>
<li>
<strong>The transition function</strong>, which is the function defining how to move an agent from one state to another based on the current state and the action it chose to perform.</li>
<li>
<strong>The reward function</strong>, which is the function defining the reward to send based on the current state and the action it chose to perform.</li>
</ul>
<p>We consider the transition function to be unknown. The state and action spaces are discrete. Besides, we assume the existence of some prior knowledge on the transition function of the MDP to be played. This prior knowledge is encoded in the form of an MDP distribution, accessible before interacting with the real MDP. During this offline-learning phase, the time consumed by the agent is monitored.</p>
<p>In order to provide a fair comparison between the different agents, different experiments are defined. Running an experiment for an agent in a given experiment aims to test this agent on the MDPs drawn from a test distribution, defined in the experiment. Each test is independent. ((The agent cannot learn from one test to improve its performances on the others.))  We monitor the time consumed by the agents during both the offline and online phases.</p>
<p>The agents are compared with respect to their performances and the time consumed during the offline and online phases.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#main-advantages" id="user-content-main-advantages"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Main Advantages</h1>
<ul>
<li>Exhaustive command-line options, allowing you to perform your experiments easily.</li>
<li>Agents, MDPs, MDP distributions and experiments can be saved in files.</li>
<li>During very long experiments, you can create backups automatically and restart from where you have stopped.</li>
<li>Every single trajectory encountered during an experiment is saved.</li>
<li>Multi-threading is supported for some implemented algorithms.</li>
<li>Your results are automatically stored in a neat Latex report, containing Gnuplot graphs and latex tables.</li>
<li>BBRL source code is well documented, allowing anyone to add their own agents/experiments.</li>
</ul>

        </div>

    </div>]