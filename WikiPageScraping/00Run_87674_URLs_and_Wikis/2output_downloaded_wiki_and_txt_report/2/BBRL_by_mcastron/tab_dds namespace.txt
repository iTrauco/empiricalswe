[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p>This namespace gathers specialised classes representing the common components in Bayesian reinforcement learning for <strong>D</strong>iscrete state space, <strong>D</strong>iscrete action space and <strong>S</strong>ingle rewards.</p>
<ul>
<li>
<strong>Agent class</strong>, represents an agent.</li>
<li>
<strong>MDP class</strong>, represents an MDP.</li>
<li>
<strong>MDPDistribution class</strong>, represents an MDP distribution.</li>
<li>
<strong>Experiment class</strong>, represents an experiment.</li>
</ul>
<h5></h5>
<ul>
<li>
<strong>simulation namespace</strong>, gathers a set of tool
s to perform an experiment.</li>
<li>
<strong>opps namespace</strong>, gathers a set of tools for applying the <strong>OPPS</strong> meta-learning scheme.</li>
</ul>
<h1>
<a aria-hidden="true" class="anchor" href="#opps-namespace-in-details" id="user-content-opps-namespace-in-details"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>opps namespace in details</h1>
<p><strong>OPPS</strong> stands for <strong>O</strong>ffline <strong>P</strong>rior-base Policy <strong>S</strong>earch.</p>
<ul>
<li>
<strong>UCB1 class</strong>, an instance of <strong>UCB1</strong> algorithm to address <em>discrete</em> strategy spaces</li>
<li>
<strong>UCT class</strong>, an instance of <strong>UCT</strong> algorithm to address <em>continuous</em> strategy spaces</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#ucb1-class" id="user-content-ucb1-class"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>UCB1 class</h2>
<p>It defines the <strong>OPPS</strong> meta-learning scheme for a discrete and finite set of agents (<strong>OPPS-DS</strong>, <strong>OPPS</strong> for <strong>D</strong>iscrete strategy <strong>S</strong>pace). It aims to use the <strong>UCB1</strong> algorithm to determine which agent of the given set, in average, is the best one in a given distribution.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#uct-class" id="user-content-uct-class"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>UCT class</h2>
<p>It defines the <strong>OPPS</strong> meta-learning scheme for a continuous set of agents (<strong>OPPS-CS</strong>, <strong>OPPS</strong> for <strong>C</strong>ontinuous strategy <strong>S</strong>pace). It aims to use the <strong>UCT</strong> algorithm to determine which agent of the given set, in average, is the best one in a given distribution. The agents have to be parameterised by a small<sup><a href="#footnotes">1</a></sup> set of parameters (floating point values). For each parameter, you have to define a lower and an upper bound.</p>
<p>To achieve this task easily, we introduced the abstract class <code>AgentFactory</code>. If you want to address a continuous set of agents, you have to specialise <code>AgentFactory</code> appropriately. To illustrate this algorithm, we already implemented several <code>AgentFactory</code>'s, such as the <code>EGreedyAgentFactory</code>, which defines a continuous set of <code>e</code>-Greedy agents, differing only by the value of <code>e</code>.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#-1" id="user-content--1"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a><a name="user-content-footnotes"></a>
</h1>
<p><sub>1: In general, at most 7 parameters.</sub></p>

        </div>

    </div>]