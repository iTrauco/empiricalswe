[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p><code>BBRL-DDS</code> gathers all the required tools to perform bayesian RL experiments. <code>DDS</code> stands for:</p>
<ul>
<li>
<strong>D</strong>iscrete state space</li>
<li>
<strong>D</strong>iscrete action space</li>
<li>
<strong>S</strong>ingle reward</li>
</ul>
<h1>
<a aria-hidden="true" class="anchor" href="#classic-use" id="user-content-classic-use"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Classic use</h1>
<h2>
<a aria-hidden="true" class="anchor" href="#step-1---offline-learning" id="user-content-step-1---offline-learning"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 1 - Offline Learning</h2>
<p>Firstly, we train our agents on the chosen prior distribution, which can be achieved through the following command-line options:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --offline_learning \
                   --agent [...] \
                   --mdp_distribution [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_agent.dat<span class="pl-pds">"</span></span></pre></div>
<p>The output file <code>out_agent.dat</code> represents the agent trained offline on the prior distribution chosen. This training is mandatory for most agents.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#step-2---create-an-experiment" id="user-content-step-2---create-an-experiment"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 2 - Create an experiment</h2>
<p>Secondly, we create an experiment that consists of a set of MDPs drawn from a chosen test distribution, with a set of parameters defining it.
This can be achieved through the following command-line options:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --new_experiment \
                   --name [...] \
                   --mdp_distribution [...] \
                   --n_mdps [...] --n_simulations_per_mdp [...] \
                   --discount_factor [...] --horizon_limit [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_experiment.dat<span class="pl-pds">"</span></span></pre></div>
<p>The output file <code>out_experiment.dat</code> represents the experiment defined.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#step-3---run-an-experiment" id="user-content-step-3---run-an-experiment"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 3 - Run an experiment</h2>
<p>Finally, we run the agent we previously trained on the created experiment. It aims to test the agent on each MDP of the experiment with respect to the parameters previously chosen<sup><a href="#footnotes">1</a></sup>.
This can be achieved through the following command-line options:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --run_experiment \
                   --experiment [...] \
                   --agent [...] \
                   --n_threads [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_results.dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency [...] --backup_frequency [...]</pre></div>
<p>The output file <code>out_results.dat</code> represents the results of the experiment for the tested agent.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#complete-usage-example" id="user-content-complete-usage-example"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Complete usage example</h2>
<p>You can find a complete usage example <a href="https://github.com/mcastron/BBRL/wiki/FAQ-(User-Guide)#3-can-you-give-a-complete-example">here</a>.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#other-uses" id="user-content-other-uses"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Other uses</h1>
<h2>
<a aria-hidden="true" class="anchor" href="#show-help" id="user-content-show-help"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Show help</h2>
<p>You can see the complete command-line options manual <a href="https://github.com/mcastron/BBRL/blob/master/doc/command-line%20manual%20(BBRL-DDS).txt">here</a> or by using the <code>--help</code> command-line option:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --help</pre></div>
<h2>
<a aria-hidden="true" class="anchor" href="#create-a-mdp-distribution" id="user-content-create-a-mdp-distribution"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Create a MDP distribution</h2>
<p>BBRL comes with six different MDP distributions. However, if you need to use other distributions, this section will help you do that.</p>
<p>An MDP distribution is defined by a transition and a reward function. The transition function is defined through a Dirichlet multinomial distribution, defining a <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution" rel="nofollow">Dirichlet distribution</a> for each state-action pair. It is parameterised by a set of weights, one per transition.</p>
<p>The reward function associates a reward to each transition, either constant or stochastic. In both cases, you should define the mean reward of each transition. In the stochastic case, you must specify the variance for each transition as well. At this time, a stochastic reward function is represented by Gaussian distributions.</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --mdp_distrib_generation \
                   --name [...] \
                   --short_name [...] \
                   --n_states [...] --n_actions [...] <span class="pl-cce">\ </span>--ini_state [...] \
                   --transition_weights [...] \
                   --reward_type [...] \
                   --reward_means [...] \
                   --reward_variances [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_distribution.dat<span class="pl-pds">"</span></span></pre></div>
<h2>
<a aria-hidden="true" class="anchor" href="#compress-your-data" id="user-content-compress-your-data"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Compress your data</h2>
<p>By adding the <code>--compress_output</code> flag, you can compress the output file. For example, if you call your output file <code>out.dat</code>, by adding this flag, <code>BBRL-DDS</code> will create the file <code>out.dat.zz</code> instead, which is a compressed version of <code>out.dat</code>.</p>
<p>Please note that a file is automatically detected as compressed by the system. Whenever you have to give a file as an argument, you can either use its compressed or uncompressed version.</p>
<p>We included a program to allow you to compress/decompress those files. See the <a href="https://github.com/mcastron/BBRL/wiki/(De)compress-your-data"><strong>(De)compress your data</strong></a> page.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#save-the-trajectories" id="user-content-save-the-trajectories"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Save the trajectories</h2>
<p>By adding the <code>--save_trajectories</code> flag, you can save the trajectories which have occurred during the experiment in the results file.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#restart-from-a-backup-file" id="user-content-restart-from-a-backup-file"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Restart from a backup file</h2>
<p>When running an experiment (<code>--run_experiment</code> flag), you will be able to create periodically a backup file by choosing a nonzero value for the <code>--backup_frequency</code> option.</p>
<p><code>BBRL-DDS</code> will create a backup file called <code>out_results.dat.bak</code> (or <code>out_results.dat.bak.zz</code> if compressed) at the chosen frequency. If <code>BBRL-DDS</code> stops working for any reason (e.g.: power cut, process killed, ...), you will be able to restart from the last saved backup.</p>
<p>On startup, <code>BBRL-DDS</code> will search for a backup file. If it succeeds in finding it, the process will start from this file. This way, you do not need to modify the initial command.</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --run_experiment \
                   --experiment \
                        --experiment_file <span class="pl-s"><span class="pl-pds">"</span>exp.dat<span class="pl-pds">"</span></span> \
                   --agent [...] \
                   --n_threads [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_results.dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency [...] --backup_frequency [...]</pre></div>
<h2>
<a aria-hidden="true" class="anchor" href="#opps-ds-and-opps-cs" id="user-content-opps-ds-and-opps-cs"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>OPPS-DS and OPPS-CS</h2>
<p><a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">OPPS-DS</a> and OPPS-CS<sup><a href="#footnotes">2</a></sup> are two algorithms whose goal is to select an <code>Agent</code> in a set. During the offline phase, those algorithms will exploit the prior MDP distribution in order to find, in a given set, the <code>Agent</code> that behaves at best in average.</p>
<p>We designed <a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">OPPS-DS</a> to address discrete sets, and we designed OPPS-CS<sup><a href="#footnotes">2</a></sup> to address continuous sets.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#discrete-set" id="user-content-discrete-set"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Discrete set</h3>
<p>In order to use <a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">OPPS-DS</a>, you will have to build a discrete set of <code>Agents</code>. In <code>BBRL-DDS</code>, this is achieved by:</p>
<ul>
<li>Generating a set of small formulas<sup><a href="#footnotes">3</a></sup>.</li>
<li>Defining, for each formula, a <code>FormulaAgent</code>.</li>
</ul>
<p>Consequently, in order to be able to define an <code>OPPSDSAgent</code>, you will have to provide a list of formulas. This way, the <code>OPPSDSAgent</code> will be able to build the corresponding set of <code>FormulaAgents</code><sup><a href="#footnotes">4</a></sup>.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#continuous-set" id="user-content-continuous-set"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Continuous set</h3>
<p>In order to use OPPS-CS<sup><a href="#footnotes">2</a></sup>, you will have to define a continuous set of <code>Agents</code>. To this end, we decided to represent a set of <code>Agents</code> by an <code>AgentFactory</code>. An <code>AgentFactory</code> is a black-box which, given a set of continuous parameters, returns the corresponding <code>Agent</code>. An <code>AgentFactory</code> represents a continuous set of <code>Agents</code> parameterised by some parameters.</p>
<p>Therefore, you will need to specify which <code>AgentFactory</code> to use when defining an <code>OPPSCSAgent</code>.</p>
<p>Everything is detailed in the command-line manual (or <a href="https://github.com/mcastron/BBRL/blob/master/doc/command-line%20manual%20(BBRL-DDS).txt">here</a>).</p>
<h3>
<a aria-hidden="true" class="anchor" href="#generate-a-set-of-formulas" id="user-content-generate-a-set-of-formulas"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Generate a set of formulas</h3>
<p>If you want to use the <a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">OPPS-DS</a> algorithm in <code>BBRL-DDS</code>, you will need to generate a set of formulas before running the offline learning phase. This is achieved by the following command-line options:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --formula_set_generation \
                   --n_variables [...] --tokens [...] --max_size [...] \
                   --reduce --n_points [...] --points_range [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_formulas.dat<span class="pl-pds">"</span></span></pre></div>
<p>The output file <code>out_formulas.dat</code> represents the set of formulas generated. The <code>--reduce</code> option is optional but strongly recommended. In short, without the reduction phase, a set of formulas contains a large number of equivalent formulas, which will penalise strongly the latter offline learning phase.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#ann-brl" id="user-content-ann-brl"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>ANN-BRL</h2>
<p>ANN-BRL<sup><a href="#footnotes">2</a></sup> is an algorithm whose goal is to train an Artificial Neural Networks for Bayesian Reinforcement Learning. During the offline phase, this algorithm will exploit the prior MDP distribution in order to many simulations and inferred Supervised Learning samples. An ANN is then trained on those samples.</p>
<p>In order to use ANN-BRL<sup><a href="#footnotes">2</a></sup>, you will have to perform simulations and generated the corresponding Supervised Learning samples<sup><a href="#footnotes">5</a></sup>.</p>
<p>Consequently, in order to be able to define an <code>ANNAgent</code>, you will have to provide a set of Supervised Learning samples. This way, the <code>ANNAgent</code> will be able to train an ANN.</p>
<p>Everything is detailed in the command-line manual (or <a href="https://github.com/mcastron/BBRL/blob/master/doc/command-line%20manual%20(BBRL-DDS).txt">here</a>).</p>
<h3>
<a aria-hidden="true" class="anchor" href="#generate-supervised-learning-samples" id="user-content-generate-supervised-learning-samples"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Generate Supervised Learning samples</h3>
<p>If you want to use the ANN-BRL<sup><a href="#footnotes">2</a></sup> algorithm in <code>BBRL-DDS</code>, you will need to generate a set of Supervised Learning samples before running the offline learning phase. This is achieved by the following command-line options:</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --sl_samples_generation \
                   --n_mdps [...] --discount_factor [...] --horizon_limit [...] \
                   --experiment --experiment_file [...] \
                   --mdp_distribution [...] \
                   --output <span class="pl-s"><span class="pl-pds">"</span>out_sl_samples.dat<span class="pl-pds">"</span></span>             </pre></div>
<p>The output file <code>out_sl_samples.dat</code> represents the set of Supervised Learning samples generated.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#" id=""><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a><a name="user-content-footnotes"></a>
</h1>
<p><sub>1: The number of simulations per MDP, the discount factor and the horizon limit.</sub><br/>
<sub>2: See note at <a href="https://github.com/mcastron/BBRL/wiki/FAQ-(User-Guide)#1-which-agents-are-implemented">this page</a> for more details.</sub><br/>
<sub>3: See <a href="https://github.com/mcastron/BBRL/wiki/Run-experiments#generate-a-set-of-formulas">this section</a> for more details.</sub><br/>
<sub>4: <code>Agents</code> based on formulas.</sub><br/>
<sub>5: See <a href="https://github.com/mcastron/BBRL/wiki/Run-experiments#generate-supervised-learning-samples">this section</a> for more details.</sub></p>

        </div>

    </div>]