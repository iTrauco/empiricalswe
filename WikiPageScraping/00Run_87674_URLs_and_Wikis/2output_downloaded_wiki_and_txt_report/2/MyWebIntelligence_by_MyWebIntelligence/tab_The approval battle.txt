[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p>Approving is the act of deciding whether a resource belongs to the territoire or not.
It's been a particularly tedious problem to solve. This page documents attempts, failures and hopes.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#approving" id="user-content-approving"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Approving</h2>
<p>Building a territoire starts by fetching the pages at the end of query result URLs.
While crawling, a lot of links to pages are discovered. Too many links to too many pages to be all visited within a reasonable amount of time. As a consequence, the pages that will be visited must be chosen and prioritized. Then, when some pages are fetched, one needs to verify whether they were actually relevant.</p>
<p>There are different sources of data about a resource to decide whether to crawl a link or not.</p>
<ul>
<li>Structure
<ul>
<li>whether a link appears from several different pages (and these pages are "important")</li>
<li>whether a link belongs to a domain that is either already in the territoire or linked to by other pages from other "important" domains</li>
</ul>
</li>
<li>Semantics
<ul>
<li>whether the page retrieved is relevant to the territoire queries (based on the present words)</li>
</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#attempts-and-failure" id="user-content-attempts-and-failure"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Attempts and failure</h2>
<h3>
<a aria-hidden="true" class="anchor" href="#naive-approach" id="user-content-naive-approach"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Naive approach</h3>
<p>Just accept anything that comes. Even with Readability, it results in ~20 links/pages, exponential growth which explodes very quickly.</p>
<p><strong>We've learnt</strong> that there are too many pages on the web to be naive about it.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#keep-only-pages-where-the-query-keywords-appear" id="user-content-keep-only-pages-where-the-query-keywords-appear"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Keep only pages where the query keywords appear</h3>
<p>This is too restrictive. Synonyms are ignored, words with the same root are ignored. This results in too big a loss to be relevant.</p>
<p><strong>We've learnt</strong> that text analysis to be relevant needs to be more subtle than finding the exact query words.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#other-ideas" id="user-content-other-ideas"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Other ideas</h2>
<h3>
<a aria-hidden="true" class="anchor" href="#definitive-architecture" id="user-content-definitive-architecture"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Definitive architecture?</h3>
<p>2.000 query results yield 50.000 URLs. It may be unreasonable to visit them all or at least, their visit should be prioritized in hope to find the most relevant content first.</p>
<ol>
<li>Prioritize structurally which URL to visit</li>
<li>Reject based on content semantics</li>
<li>Approve</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#random-notes" id="user-content-random-notes"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>random notes</h2>
<ul>
<li>Use elastic search to build a (stem?) dictionary of the current corpus. Evaluate new documents against this dictionary</li>
<li>Maintain the domain graph and choose to never visit pages from domains with <code>in-degree = 1</code>.</li>
<li>When a territoire depth = 0 is done, get the graph, take the first 100 (or 500 or 1000) most relevant pages, get them to increase the territoire, and redo the graph, find the next most relevant in the new graph, etc. This takes advantage of <a href="https://en.wikipedia.org/wiki/Bradford's_law" rel="nofollow">Bradford's law</a>
</li>
</ul>

        </div>

    </div>]