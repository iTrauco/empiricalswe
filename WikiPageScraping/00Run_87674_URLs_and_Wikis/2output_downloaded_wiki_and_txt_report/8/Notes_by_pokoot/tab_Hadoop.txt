[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p>EMR = Elastic Map Reduce from Amazon (Cloud Solution)</p>
<h2>
<a aria-hidden="true" class="anchor" href="#hadoop-technology-stack" id="user-content-hadoop-technology-stack"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Hadoop Technology Stack:</h2>
<ul>
<li>Hadoop Core
<ul>
<li>HDFS</li>
<li>Mapreduce</li>
<li>Yarn 2.0</li>
</ul>
</li>
<li>Data Acess
<ul>
<li>Pig</li>
<li>Hive (HiveQL)</li>
</ul>
</li>
<li>Data Storage
<ul>
<li>HBase (Base on Google's Big Table)</li>
<li>Cassandra</li>
<li>Mongodb</li>
</ul>
</li>
<li>Interaction Visualizatoin Execution Development
<ul>
<li>HCatalog (Meta Table - Shared Schema)</li>
<li>Lucene (Full Text Searching)</li>
<li>Hama  (Books Synchronize)</li>
<li>Crunch (Map reduce pipelines)</li>
</ul>
</li>
<li>Data Serialization
<ul>
<li>Avro</li>
<li>Thrift</li>
</ul>
</li>
<li>Data Intelligence
<ul>
<li>Drill</li>
<li>Mahout</li>
</ul>
</li>
<li>Data Integration
<ul>
<li>Sqoop</li>
<li>Flume</li>
<li>Chukwa</li>
</ul>
</li>
<li>Management, Monitoring, Orchestration
<ul>
<li>Zookeeper</li>
<li>Ambari</li>
<li>Oozie</li>
</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#incubator-projects" id="user-content-incubator-projects"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Incubator Projects:</h2>
<pre><code>http://incubator.apache.org/projects/ 
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#browse-hadoop-common-urls" id="user-content-browse-hadoop-common-urls"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Browse Hadoop Common URL's</h2>
<pre><code>(Name Node) http://hnname:50070
(Job Tracker) http://hnname:50030
(Task Tracker) http://hnname:50060
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#mapreducer-example" id="user-content-mapreducer-example"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Mapreducer Example</h2>
<pre><code>$ hadoop/bin/hadoop fs -mkdir /data
$ hadoop/bin/hadoop fs -copyFromLocal words.txt /data
$ hadoop/bin/hadoop jar hadoop/hadoop*examples*.jar wordcount /data/words.txt /data/resuts
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#installation-single-node-cluster" id="user-content-installation-single-node-cluster"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Installation Single Node Cluster</h2>
<pre><code>$ sudo apt-get install openssh-server
$ ssh hnname

@ Name Node
// Authorize and create ssh keys

$ ssh-keygen
$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
$ ssh localhost 

// Copy keys to our client server
$ ssh-copy-id -i ~/.ssh/id_rsa.pub web@hncclient
$ ssh hnclient

// Install Java 7 (Prefered)
$ sudo apt-get install openjdk-7-jdk
$ java -version

// Install Java 7 SE (Optional)
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get udpate
$ sudo apt-get install oracle-java7-installer

// Download hadoop   
$ wget http://mirror.nus.edu.sg/apache/hadoop/common/hadoop-1.2.1/hadoop-1.2.1-bin.tar.gz
$ tar -zxvf hadoop-1.2.1-bin.tar.gz
$ sudo cp -r hadoop-1.2.1 /usr/local/hadoop

// Edit .bashrc
$ sudo vi $HOME/.bashrc

export HADOOP_PREFIX=/usr/local/hadoop
export PATH=$PATH:$HADOOP_PREFIX/bin

save it.

// Execute and check Path
$ exec bash
$ $PATH

// Change hadoop configuration file
$ sudo vim /user/local/hadoop/conf/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=True

Save it

// Disable Ipv6
$ sudo vi /etc/sysctl.conf

net.ipv6.conf.alldisable_upv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.Io.disable_ipv6 = 1 

 Save it.

// Edit Core Site
$ sudo vim /user/local/hadoop/conf/core-site.xml

&lt;configuration&gt;
    &lt;property&gt;  
        &lt;name&gt;fs.default.name&lt;/name&gt;
        &lt;value&gt;hdfs://HNName:10001&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;  
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;

// Edit Mapred-site.xml

$ sudo vim /user/local/hadoop/conf/mapred-site.xml

&lt;configuration&gt;
    &lt;property&gt;  
        &lt;name&gt;mapred.job.tracker&lt;/name&gt;
        &lt;value&gt;HNName:10002&lt;/value&gt;
    &lt;/property&gt;      
&lt;/configuration&gt;

Save it

// Create the tmp directory
$ sudo mkdir /usr/local/hadoop/tmp    
$ sudo chown web /usr/local/hadoop/tmp 
  
// Format name node
$ hadoop namenode -format

// Fire hadoop
$ start-all.sh 

// Check if node is working
$ jps 

// Hadoop administration commands
$ stop-all.sh
$ hadoop-daemons.sh start namenode 
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#installing-hadoop-multi-node" id="user-content-installing-hadoop-multi-node"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Installing Hadoop Multi Node</h2>
<pre><code> // Assumtions
 - HNName (Name Node and Job Tracker)
 - HN2ndName (Secondary Node)
 - HNData1
 - HNData2
 - HNData3
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#hnname" id="user-content-hnname"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>@HNName</h2>
<pre><code> // Copy Ssh keys
 $ ssh-copy-id i $HOME/.ssh/id_rsa.pub web@HNData1
 $ ssh-copy-id i $HOME/.ssh/id_rsa.pub web@HNData2
 $ ssh-copy-id i $HOME/.ssh/id_rsa.pub web@HNData3
 $ ssh-copy-id i $HOME/.ssh/id_rsa.pub web@HN2ndName

 // Test keys from HNName
 $ ssh HNData1
 $ ssh HNData2
 $ ssh HNData3
 $ ssh HN2ndName

 // Edit masters and Slave
 
 $ sudo vi /usr/local/hadoop/conf/masters

 HN2ndName    

 $ sudo vi /usr/local/hadoop/conf/slaves

 HNData1
 HNData2
 HNData3


$ hadoop namenode -format  
$ start-dfs.sh
$ start-mapred.sh
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#hndata1-hndata2-hndata3" id="user-content-hndata1-hndata2-hndata3"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>@HNData1, @HNData2, @HNData3</h2>
<pre><code> // Point task tracker to job tracker (mapred.site.xml)
 // Point data nodes to name nodes (core-site.xml)
 
 $ sudo vi /usr/local/hadoop/conf/core-site.xml 

 &lt;configuratoin&gt;
     &lt;property&gt;
         &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://HNName:10001&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
         &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;
     &lt;/property&gt; 
 &lt;/configuration&gt;


 $ sudo vi /usr/local/hadoop/conf/mapred-site.xml 

 &lt;configuratoin&gt;
     &lt;property&gt;
         &lt;name&gt;mapred.job.tracker&lt;/name&gt;
         &lt;value&gt;HNNAME:10002&lt;/value&gt;
     &lt;/property&gt;
 &lt;/configuration&gt;
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#decommission-a-node" id="user-content-decommission-a-node"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Decommission A Node</h2>
<pre><code>$ sudo touch /usr/local/hadoop/excludes
$ sudo vi /usr/local/hadoop/excludes

HNData3

$ sudo vi /usr/local/hadoop/conf/core-site.xml

Add the following : 
...
&lt;property&gt;
  &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;
  &lt;value&gt;/usr/local/hadoop/exludes/&lt;/value&gt;
&lt;/propery&gt;

$ hadoop dfsadmin -refreshNodes 

// Validate the decommissioned node

http://hnname:50070/dfshealth.jsp

// Start the balancer
$ start-balancer.sh 
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#troubleshooting-hadoop" id="user-content-troubleshooting-hadoop"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Troubleshooting Hadoop</h2>
<pre><code>$HADOOP_PREFIX/logs

.out file are startup information
.log file are log file information

On web browser:
hndata3:50075/logs

Check for corruption
$ fsck
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#data-data-data-sample-datas" id="user-content-data-data-data-sample-datas"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Data Data Data, Sample datas</h2>
<ul>
<li><a href="http://www.gutenberg.org" rel="nofollow">http://www.gutenberg.org</a></li>
<li><a href="http://aws.amazon.com/datasets" rel="nofollow">http://aws.amazon.com/datasets</a></li>
<li><a href="http://en.wikipedia.org/wiki/Wikipedia:Database_download" rel="nofollow">http://en.wikipedia.org/wiki/Wikipedia:Database_download</a></li>
<li><a href="http://www.infochimps.com/" rel="nofollow">http://www.infochimps.com/</a></li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#pig-latin" id="user-content-pig-latin"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Pig Latin</h2>
<p>High level data flow scripting language</p>
<p>Load -&gt; Transform -&gt; Result</p>
<pre><code>custs = LOAD `/data/big/customers` AS (cust_id, region, name);
sales = LOAD `/data/big/sales` AS (sale_id, cust_id, date, amount);
salesNA = FILTER custs BY region == 'NORTH AMERICA'
joined = JOIN custs BY cust_id, salesNA by cust_id;
grouped = GROUP joined by cust_id;
summed = FOREACH grouped GENERATE GROUP, 
         SUM(joined.salesNA::amount)
bigSpenders = FILTER summed BY $1 &gt; 100000;
sorted = ORDER bigSpender BY $1 DESC
DUMP sorted;
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#hive" id="user-content-hive"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Hive</h2>
<p>Installation</p>
<pre><code>wget http://www.motorlogy.com/apache/hive/hive-0.13.1/apache-hive-0.13.1-bin.tar.gz
tar -zxvf apache-hive-0.13.1-bin.tar.gz
sudo mv apache-hive-0.13.1 /usr/local/hive
export HIVE_PREFIX=/usr/local/hive
export PATH=$PATH:$HIVE_PREFIX/bin
hive    
</code></pre>
<p>Example</p>
<pre><code>hive &gt; CREATE TABLE book(word STRING)
     &gt; ROW FORMAT DELIMITED
     &gt; FIELDS TERMINATED BY ' ' 
     &gt; LINES TERMINATED BY '\n';

hive &gt; LOAD DATA INPATH 'hdfs:/data/small/data.txt' INTO TABLE book;

hive &gt; describe book;

hive &gt; select * from book;
hive &gt; select count(*) from book;
hive &gt; select lower(word), count(*) as count 
       from book 
       where lower(substring(word, 1, 1)) = 'w'
       group by word
       having count &gt; 50 
       sort by count desc;
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#hbase" id="user-content-hbase"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>HBASE</h2>
<p>Installation</p>
<pre><code>wget http://mirrors.advancedhosters.com/apache/hbase/hbase-0.98.6/hbase-0.98.6-hadoop1-bin.tar.gz
tar -zxvf hbase-0.98.6-hadoop1-bin.tar.gz
sudo mv hbase-0.98.6-hadoop1-bin
export HBASE_PREFIX=/usr/local/hbase
export PATH=$PATH:$HBASE_PREFIX/bin
</code></pre>
<p>Configuration</p>
<pre><code>Change java path
sudo vi $HBASE_PREFIX/conf/hbase-env.sh


sudo vi $HBASE_PREFIX/conf/hbase-site.xml

&lt;configuration&gt;
   &lt;property&gt;
       &lt;name&gt;hbase.rootdir&lt;/name&gt;
       &lt;value&gt;hdfs://HNName:10001/hbase&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
       &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
       &lt;value&gt;HNZookeeper,HNZookeeper2&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
       &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
       &lt;value&gt;false&lt;/value&gt;
   &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Running</p>
<pre><code> http://hnhnmaster:60010

 start-hbase.sh
 local-regionservers.sh start 1 2 3 

 hbase shell

 create 'htest', 'cf'
 put 'htest', 'r1' , 'cf:c1', 'v1'
 put 'htest', 'r2' , 'cf:c2', 'v2'
 put 'htest', 'r3' , 'cf:c3', 'v3'

 scan 'htest

 get 'htest', 'r2'

 // update 
 put 'htest', 'r2' 'cf:c2' , 'v2updated' 

 // delete
 delete 'htest', 'r3', 'cf:c3'

 // drop table
 disable 'htest'
 drop 'htest' 
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#sqoop" id="user-content-sqoop"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Sqoop</h2>
<p>A transferring tool from HDFS data to SQl or the other way around.</p>
<pre><code> sqoop list-databases --connect "jdbc:mysql://localhost" -username root --password root 

 //importing from relational to hdfs
 sqoop import --connect jdbc:mysql://hnclient/demodb --username root --password root --table Movies --target-dir /data/small/movies 

 // importing from relational &gt; hdfs &gt; hive 
 sqoop import --connect "jdbc:sqlserver://192.168.0.12:14323;database=demodb" --username sa --password root --table Movies --hive-import  --columns "Name, Year, Rating" -m 1

 // Export
 sqoop export --connect "jdbc:sqlserver://192.168.0.12:1433;database=demodb" --username sa --password root --export-dir /data/big/output/part-00000 --table HadoopResults --input-fields-terminated-by '\t'
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#cloudera" id="user-content-cloudera"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Cloudera</h2>
<pre><code> http://www.cloudera.com/content/support/en/downloads.html

 http://www.cloudera.com/content/support/en/downloads/quickstart_vms/cdh-5-1-x1.html
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#amazon-emr" id="user-content-amazon-emr"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Amazon EMR</h2>
<pre><code> EC2 for processing
 S3 for data storage
 EMR Elastic Map Reduce

 http://aws.amazon.com
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#microsoft-hdinsight" id="user-content-microsoft-hdinsight"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Microsoft HDInsight</h2>
<pre><code>HD - Hadoop Distribution


https://azure.microsoft.com/en-us/pricing/free-trial/
</code></pre>

        </div>

        <div class="wiki-footer gollum-markdown-content boxed-group" id="wiki-footer">
          <div class="boxed-group-inner wiki-auxiliary-content markdown-body">
            <p>Author: Harold Kim Cantil</p>

          </div>
        </div>
    </div>]