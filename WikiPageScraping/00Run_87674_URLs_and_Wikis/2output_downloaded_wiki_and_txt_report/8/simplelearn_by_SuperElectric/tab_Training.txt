[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <p><code>Sgd</code> runs the training loop and calls callbacks. On each loop, it:</p>
<ol>
<li>Pulls a new data batch from the dataset iterator.</li>
<li>Updates the values being optimized.</li>
<li>Calls <code>on_batch()</code> on any registered <code>Monitor</code> callback objects. This is useful for visualizing weights as they change over training, or checking parameters for NaNs.</li>
<li>If the iterator completes a loop through the data, calls <code>on_epoch()</code> on any registered <code>EpochCallback</code> objects.</li>
</ol>
<p>Currently, only momentum-based SGD (with and without Nesterov accelerated gradients) is implemented. To implement your own gradient-based update scheme, subclass <code>SgdParameterUpdater</code>.</p>
<p>It is possible to optimize with respect to the input instead of the parameters. Much of the workflow above is unchanged, except for the part about over a dataset. See simplelearn/examples/mnist_visualizer.py for a full example.</p>

        </div>

        <div class="wiki-footer gollum-markdown-content boxed-group" id="wiki-footer">
          <div class="boxed-group-inner wiki-auxiliary-content markdown-body">
            <p>Copyright (c) 2015 Matthew Koichi Grimes</p>

          </div>
        </div>
    </div>]