[<div class="wiki-body gollum-textile-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#%EC%B5%9C%EC%A2%85%EB%B3%B4%EA%B3%A0%EC%84%9C" id="user-content-최종보고서"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>최종보고서</h1>
<p>넥슨 분산된 key-value 저장소<br/>
프로젝트 Team J</p>
<p>컴퓨터공학부 2008-11572 김재찬<br/>
컴퓨터공학부 2008-11662 유재성<br/>
컴퓨터공학부 2012-11256 염지혜</p>
<p>엄현상 교수님</p>
<p>{{toc}}</p>
<h2>
<a aria-hidden="true" class="anchor" href="#1-abstract" id="user-content-1-abstract"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>1. Abstract</h2>
<p>Key-value 저장소는 저장 데이터베이스 관리 시스템의 가장 간단한 형태이다. Key-value 쌍들을 저장할 수 있을 뿐 아니라 key로 그 value를 검색할 수 있다. 이런 시스템은 일반적으로 복잡한 응용 프로그램에는 적합하지 않다. 하지만 특정 상황에서는 간단함은 큰 매력이 될 수 있다. 예를 들어 효율성이 중시되는 모바일, 내장형 시스템 또는 고성능 프로세스 데이터베이스에 적용될 수 있다. <a href="http://db-engines.com/en/article/Key-value+Stores" rel="nofollow">*</a></p>
<p>이런 key-value 저장소를 좀 더 적극적으로 이용하기 위해 분산시스템으로 구성한다. 분산시스템은 값싼 CPU를 조합하여 고가의 단일 CPU보다 우수한 성능의 시스템을 제공할 수 있다. 또, 일을 분산하면 단일 속도보다는 전체 연산능력을 향상될 수 있다. 또, 시스템 교체를 통한 성장성과 부분 시스템의 고장으로 인해 전체 시스템이 중단되는 신뢰성을 확보할 수 있다. 단, 이런 이점을 얻기 위해서 분산시스템을 운영하기 위한 소프트웨어가 필요하고, 네트워크 통신망 문제나 데이터 분산으로 인한 자료의 안정성에 문제가 생길 수 있다. <a href="http://www.reportworld.co.kr/report/data/view.html?no=130840" rel="nofollow">*</a></p>
<p>위와 같은 네트워크 문제나 자료 안전성 문제를 해결하면서 분산시스템에서 key-value 저장소를 운영하기 위해 분산된 key-value 저장소를 직접 구현했다. 분산 환경을 구성하기 위해 Apache ZooKeeper를 이용하여 수백 개의 Node를 관리할 수 있게 하였고, Consistent hashing 알고리즘을 사용하여 서비스를 중단하지 않고 Node를 추가/제거할 수 있게 하였다. 또, 안정성을 확보하기 위해 Master/slave model을 도입했다.</p>
<p>이렇게 구현한 저장소를 쉽게 사용하기 위해 client library를 구현했다. Client library에서는 다양한 타입의 key와 value 형식을 제공하여 편의성을 도모하였다. 그리고 이 모든 시스템을 테스트하기 위해 웹서비스를 통해 test client를 구현하고 모니터링 서비스까지 제공한다.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#2-introduction" id="user-content-2-introduction"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>2. Introduction</h2>
<ul>
	<li>프로젝트 수행달성의 배경 및 중요성
	<ul>
		<li>Key-value 저장소의 중요성<br/>
트랜잭션과 같은 일반적인 서비스 구현에 필요한 기능들이 전혀 준비되어 있지 않기 때문이다. 사실 트랜잭션이나 인덱싱이 필요하지 않거나 중요하지 않은 특수한 시스템을 구현한다고 한다면, NoSQL에서 제공되는 기능으로도 충분한 것이다. 오히려 NoSQL을 사용하는 것이 시스템 구현에 있어서 편리한 부분이 있을 수 있다. <a href="http://eincs.com/2012/06/nosql-is-not-useful/" rel="nofollow">*</a>
</li>
		<li>분산 저장소 시장의 성장과 인 메모리 기반 기술의 발전<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/963/%EB%B9%85%ED%85%8C%EC%9D%B4%ED%84%B0_%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80.jpg" src="https://camo.githubusercontent.com/a0fbe1b66388590c6ae308d23799a6efd0a94bf0/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3936332f2545422542392538352545442538352538432545432539442542342545442538342542305f2545432538412541342545442538362541302545422541362541432545432541372538302e6a7067"/><br/>
한국 <acronym title="www.idckorea.com"><span>IDC</span></acronym>가 최근 발표한 국내 빅데이터 스토리지 시장 전망 보고서(Korea Storage in Big Data 2014-2018 Forecast, Doc #KR26101453)를 보면, 올해 국내에서 빅데이터 환경 구현을 위해 도입한 스토리지 인프라 시장은 347억 원으로 추산되며, 앞으로 5년간 연평균 성장률 31.3%로 증가해 오는 2018년 1,087억 원에 이를 것으로 보인다.<br/>
하드웨어와 소프트웨어를 포함한 스토리지 제품 시장은 연평균 성장률 27.2%, 클라우드 서비스와 스토리지 전문 서비스를 포함하는 빅데이터 스토리지 서비스 시장은 연평균 40.2%의 비교적 높은 성장률을 기록할 전망이다.<br/>
보고서를 보면, 많은 기업이 빅데이터 분석에 관해 관심을 높이고 있지만, 전사적으로 고도의 빅데이터 분석 환경을 설계/구축하고 관리할만한 역량이나 예산은 부족한 상황이다. 이와 함께 대용량 데이터를 빠르게 분석하고 처리하기 위한 인 메모리 기반 기술과 올 플래시 스토리지의 활용도 확대될 것으로 보인다. <a href="http://it.donga.com/19787/" rel="nofollow">*</a>
</li>
		<li>PaaS 성장<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/965/paas_%EC%84%B1%EC%9E%A5.jpg" src="https://camo.githubusercontent.com/3ca9578bf18ca99196d470d95e7fd067f7c64fab/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3936352f706161735f2545432538342542312545432539452541352e6a7067"/><br/>
PaaS의 성장은 컴퓨팅 성능, 스토리지 및 네트워킹뿐 아니라 데이터베이스, 파일 공유 및 기타 기능 등 애플리케이션 개발에 필요한 주요 구성요소를 개발자에게 제공함으로써 플랫폼에서 실행될 특정 요구 소프트웨어 요소의 제작에 집중할 자원을 가용할 수 있게 해주기 때문에 가능했다고 생각합니다. <a href="http://www.akamai.co.kr/html/solutions/perf_sec_challenges.html" rel="nofollow">*</a>
</li>
	</ul>
</li>
	<li>접근방법
	<ul>
		<li>분산 환경<br/>
Apache ZooKeeper를 이용하여 분산된 다수의 Node를 관리한다. 이렇게 구성된 Node들에 대량의 key-value 데이터를 균형 있게 저장한다. ZooKeeper는 각 노드의 변경 사항을 관리하고 노드들끼리 싱크를 맞춰준다.</li>
		<li>실시간<br/>
Consistent hashing 알고리즘을 사용하여 서비스를 중단하지 않고 Node를 추가/제거할 수 있게 하였고, key-value 데이터 균형을 유지한다. Consistent hashing은 하나의 노드가 일정 범위의 해시값을 가지는 데이터들을 각각 처리하도록 하는 모델이다. 이 모델에서 노드가 추가되면 인접한 다른 노드에서 적절한 양의 데이터를 할당받고, 노드가 제거되면 마찬가지로 인접한 다른 노드들이 제거된 노드의 데이터를 적절히 나눠 할당받기 때문에 다른 노드들은 영향을 받지 않는다는 장점이 있다.</li>
		<li>안전한 저장소<br/>
안전한 저장소를 구현하기 위해 Master/slave model을 도입하여 데이터 사본을 구현하였다. 일부 노드가 정상 작동하지 않아도 전체 시스템이 정상적으로 작동되어야 하므로 노드마다 해당 노드의 데이터를 백업하고, 복원할 방법이 있어야 하기 때문이다. 보통은 Master가 Overlords와 통신을 하고 slave는 Master의 데이터를 주기적으로 백업하여 저장하도록 했다. Master가 정상작동하지 않으면 Overlords는 slaves 중에 하나를 선택하여 Master의 임무를 수행하도록 한다.</li>
		<li>이용 편의성<br/>
저장소에 접근하기 위한 클라이언트 라이브러리를 구현하였고 이 라이브러리를 이용하여 간단한 클라이언트와 모니터링 클라이언트를 구현하였다.</li>
		<li>통신 계층<br/>
Overlord와 Node 간의 네트워크 통신, Node와 Node 간의 네트워크 통신, Overlord와 Client 간의 네트워크 통신이 필요하다. 네트워크 통신을 위한 저수준의 통신을 직접 구현하지 않고 ZeroMQ라는 Message Queue를 사용해 통신 구현을 쉽게 할 수 있도록 한다.</li>
	</ul>
</li>
	<li>보고서의 구성
	<ul>
		<li>Background Study<br/>
기존의 in-memory database와 distribution system들의 기능과 한계점을 분석한다.</li>
		<li>Goal/Problem &amp; Requirements<br/>
위에서 파악한 문제점들을 보안한 우리의 프로젝트 목표를 설정하고, 요구사항을 파악한다.</li>
		<li>Approach<br/>
설정한 목표와 파악한 요구사항을 바탕으로 이것들을 해결하기 위한 우리 프로젝트에서 사용한 방법들을 개론한다.</li>
		<li>Project Architecture<br/>
프로젝트에서 구현한 key-value 저장소의 시스템 구성을 그림으로 보이고 설명한다.</li>
		<li>Implementation Spec<br/>
프로젝트에서 구현한 요구조건들을 보이고 입출력 인터페이스, 모듈간 인터페이스, 모듈의 동작을 설명한다.</li>
		<li>Solution<br/>
프로젝트 구현의 상세 내용에 대해 구체적으로 다룬다.</li>
		<li>Result<br/>
프로젝트 구현 결과들에 대해서 서술하고 한계점에 대해 다룬다.</li>
		<li>Division &amp; Assignment of Work<br/>
프로젝트 업무 분배와 담당에 대해 기술한다.</li>
		<li>Conclusion<br/>
프로젝트 전반에 대해 마무리한다.</li>
		<li>User Manual<br/>
프로젝트에서 구현한 소프트웨어 실행 방법을 서술한다.</li>
	</ul>
</li>
	<li>용어 정의
	<ul>
		<li>Key-value store : Key-value (KV) stores use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.(http://en.wikipedia.org/wiki/NoSQL#Key-value_stores)</li>
		<li>Relational database : A relational database is a database that stores information about both the data and how it is related.(http://en.wikipedia.org/wiki/Relational_database)</li>
		<li>NoSQL : A NoSQL or Not Only <span>SQL</span> database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases.(http://en.wikipedia.org/wiki/NoSQL)</li>
		<li>Distributed data store : A distributed data store is a computer network where information is stored on more than one node, often in a replicated fashion.(http://en.wikipedia.org/wiki/Distributed_data_store)</li>
		<li>Consistent hashing : Consistent hashing is a special kind of hashing such that when a hash table is resized and consistent hashing is used, only K/n keys need to be remapped on average, where K is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped.(http://en.wikipedia.org/wiki/Consistent_hashing)</li>
		<li>Partition tolerance : the system continues to operate despite arbitrary message loss or failure of part of the system(http://en.wikipedia.org/wiki/CAP_theorem)</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#3-background-study" id="user-content-3-background-study"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>3. Background Study</h2>
<ul>
	<li>관련 접근방법/기술 장단점 분석<br/>
본 프로젝트는 크게 In Memory Storage 와 Distributed System 이라는 두가지 키워드에 집중한다.
	<ul>
		<li>In Memory Storage
		<ul>
			<li>In-memory databases<br/>
메모리를 저장소로 쓰는 데이터베이스는 디스크를 저장소로 쓰는 데이터베이스에 비해 성능이 크게 뛰어나다. 하지만 아직까지 분산처리 기능을 완벽히 지원하는 In memory storage product가 없어 분산처리 환경에서 사용하기엔 아직 불편하다.
			<ul>
				<li>
<span>ACID</span> support<br/>
In their simplest form, main memory databases store data on volatile memory devices. These devices lose all stored information when the device loses power or is reset. In this case, IMDBs can be said to lack support for the “durability” portion of the <span>ACID</span> (atomicity, consistency, isolation, durability) properties. Volatile memory-based IMDBs can, and often do, support the other three <span>ACID</span> properties of atomicity, consistency and isolation.
				<ul>
					<li>Many IMDBs have added durability via the following mechanisms<br/>
Snapshot files, or, checkpoint images, which record the state of the database at a given moment in time. The system typically generates these periodically, or at least when the <span>IMDB</span> does a controlled shut-down. While they give a measure of persistence to the data (in that the database does not lose everything in the case of a system crash) they only offer partial durability (as ’recent" changes will be lost). For full durability, they need supplementing with one of the following:</li>
					<li>Transaction logging, which records changes to the database in a journal file and facilitates automatic recovery of an in-memory database.<br/>
Non-Volatile <span>DIMM</span> (<span>NVDIMM</span>), a memory module that has a <span>DRAM</span> interface, often combined with <span>NAND</span> flash for the Non-Volatile data security. The first <span>NVDIMM</span> solutions were designed with supercapacitors instead of batteries for the backup power source. With this storage, <span>IMDB</span> can resume securely from its state upon reboot.</li>
					<li>Non-volatile random access memory (<span>NVRAM</span>), usually in the form of static <span>RAM</span> backed up with battery power (battery <span>RAM</span>), or an electrically erasable programmable <span>ROM</span> (<span>EEPROM</span>). With this storage, the re-booting <span>IMDB</span> system can recover the data store from its last consistent state.</li>
					<li>High availability implementations that rely on database replication, with automatic failover to an identical standby database in the event of primary database failure. To protect against loss of data in the case of a complete system crash, replication of a <span>IMDB</span> is normally supplements one or more of the mechanisms listed above.<br/>
Some IMDBs allow the database schema to specify different durability requirements for selected areas of the database – thus, faster-changing data that can easily be regenerated or that has no meaning after a system shut-down would not need to be journaled for durability (though it would have to be replicated for high availability), whereas configuration information would be flagged as needing preservation.</li>
				</ul>
</li>
			</ul>
</li>
			<li>List of in-memory databases
			<ul>
				<li>MemSQL</li>
				<li>EXASolution</li>
				<li>WebDNA</li>
				<li>Oracle Exalytics</li>
				<li>H2 (<span>DBMS</span>)</li>
				<li>Oracle Coherence</li>
				<li>Hazelcast</li>
				<li>
<span>SAP</span> <span>HANA</span>
</li>
				<li>Datablitz</li>
				<li>Polyhedra</li>
				<li>Kognitio Analytical Platform</li>
				<li>Ehcache	Terracotta, Inc. (Software AG)</li>
				<li>타임스텐	오라클</li>
				<li>알티베이스 <span>HDB</span>
</li>
				<li>마이크로소프트 COM+ <span>IMDB</span>
</li>
				<li>eXtremeDB</li>
				<li>solidDB</li>
				<li>VoltDB</li>
				<li>BigMemory</li>
				<li>Xeround</li>
				<li>SQLFire</li>
				<li>ActiveSpaces</li>
				<li>마이크로소프트 <span>SQL</span> 서버</li>
				<li>UnQLite</li>
			</ul>
</li>
			<li>In memory dictionary Redis<br/>
In-memory에서 가장 널리 쓰이고 있는 redis의 정식버전은 아직 분산처리를 지원하지 않고 있다. 레디스는 홈페이지를 통해 자신들의 분산처리 계획에 대해 서술하고 있다.(<a href="http://redis.io/topics/partitioning" rel="nofollow">Partitioning: how to split data among multiple Redis instances.</a>)</li>
		</ul>
</li>
		<li>Distributed System
		<ul>
			<li>Distributed databases are usually non-relational databases that make a quick access to data over a large number of nodes possible. Some distributed databases expose rich query abilities while others are limited to a key-value store semantics. As the ability of arbitrary querying is not as important as the availability, designers of distributed data stores have increased the latter at an expense of consistency. But the high-speed read/write access results in reduced consistency, as it is not possible to have both consistency, availability, and partition tolerance of the network, as it has been proven by the <span>CAP</span> theorem.</li>
			<li>분산 파일 시스템에는 일반적으로 투명성이 보장되므로 네트워크를 통해 접근하는 파일들은 프로그램과 사용자의 로컬 디스크에 있는 파일처럼 다룰 수 있다. 저장하고 처리 해야 할 데이터의 양이 늘어남에 따라 효율적인 분산 처리 방법에 대한 연구가 활발히 진행되고 있는데, 데이터베이스의 경우 부하를 줄이기 위해 scale-out storage를 많이 사용한다. scale-out storage는 여러개의 노드를 저장소로 쓰고, 데이터 처리량을 적절히 나누어 각각의 저장소에 가해지는 부하가 균형있게 배분되도록 하는 저장소이다. 이 때 데이터 처리를 나누기 위해 많이 쓰는 방법이 sharding 이다. 노드 갯수를 정해놓고, static sharding을 하는 것이 구현하기 간단하기 때문에 널리 쓰이고 있다. 그러나 static sharding은 데이터 처리량에 따라 노드를 추가하거나 제거할 때 데이터를 재배치해야 하기 때문에 비용이 크고, 동적으로 수행할 수 없다는 단점이 있다.</li>
			<li>Distributed non-relational databases
			<ul>
				<li>Apache Cassandra, former data store of Facebook</li>
				<li>BigTable, the data store of Google</li>
				<li>Druid (open-source data store), used by Netflix</li>
				<li>Dynamo of Amazon</li>
				<li>HBase, current data store of Facebook’s Messaging Platform</li>
				<li>Riak</li>
				<li>Voldemort, data store used by LinkedIn</li>
			</ul>
</li>
		</ul>
</li>
		<li>Choosing Storage
		<ul>
			<li>O(1)<br/>
Redis가 최선의 선택입니다.</li>
			<li>O(n), O(n*t)<br/>
몇가지 대안이 있습니다.
			<ul>
				<li>HBase
				<ul>
					<li>장점:
					<ul>
						<li>요구사항에 가장 부합함</li>
						<li>관리가 쉬움 ( 분산 파일 시스템위에 스토리지가 구축되어 있고, 서버마다 데이터가 ad hoc 하게 파티션되어있음)</li>
					</ul>
</li>
					<li>단점:
					<ul>
						<li>랜덤 읽기와 삭제가 느림.</li>
						<li>좀 낮은 가용성 (<span>SPOF</span> 도 약간 존재.)</li>
					</ul>
</li>
				</ul>
</li>
				<li>Cassandra
				<ul>
					<li>장점:
					<ul>
						<li>최근 데이터를 다루는데 적합함</li>
						<li>높은 가용성 (비중앙집중적 아키텍처(P2P), rack/DC 를 고려한 리플리케이션)</li>
					</ul>
</li>
					<li>단점:
					<ul>
						<li>낮은 일관성으로 인한 운영 비용 증가</li>
						<li>카운터 증가가 좀 느림.</li>
					</ul>
</li>
				</ul>
</li>
				<li>MongoDB
				<ul>
					<li>장점:
					<ul>
						<li>Auto sharding, auto failover</li>
						<li>많은 명령어</li>
					</ul>
</li>
					<li>단점:
					<ul>
						<li>타임라인의 워크로드에 부적합함 (B-tree indexing)</li>
						<li>디스크와 네트웍 사용이 비효과적임</li>
					</ul>
</li>
				</ul>
</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
</li>
	<li>프로젝트 개발환경
	<ul>
		<li>Os
		<ul>
			<li>Linux(Ubuntu 14.04)</li>
		</ul>
</li>
		<li>Programming Language
		<ul>
			<li>Python 3<br/>
For flexible type system</li>
		</ul>
</li>
		<li>Library
		<ul>
			<li>ZeroMQ<br/>
For end-to-end communication</li>
			<li>Zookeeper<br/>
Managing Nodes</li>
			<li>Flask<br/>
For Dimint Test Client</li>
		</ul>
</li>
		<li>Source repository &amp; issue tracking
		<ul>
			<li>GitHub code</li>
			<li>GitHub issues</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#4-goalproblem--requirements" id="user-content-4-goalproblem--requirements"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>4. Goal/Problem &amp; Requirements</h2>
<ul>
	<li>프로젝트의 목적<br/>
본 프로젝트에서는 <strong>분산된 환경</strong>에서	*대량*의 <strong>key-value 데이터</strong>를 <strong>동적으로 밸런싱</strong>하여 *메모리*에 저장하는 저장소를 구현하고자 한다.</li>
	<li>프로젝트 요구사항(회사 평가표를 바탕으로)
	<ul>
		<li>Key-value 스토리지의 구현
		<ul>
			<li>다양한 형태의 key에 대해 지원한다. (int, float, string)</li>
			<li>다양한 형태의 value에 대해 지원한다. (Set, list, dict)</li>
		</ul>
</li>
		<li>분산 스토리지에 대한 구현
		<ul>
			<li>스토리지 서비스를 중단하지 않고 node의 개수를 늘릴 수 있다.</li>
			<li>최소 10개 이상의 node로 구성할 수 있다.</li>
		</ul>
</li>
		<li>분산 스토리지의 완성도
		<ul>
			<li>서비스 안전성<br/>
몇 개의 Node가 죽어도 다른 데이터들에 정상적으로 접근할 수 있어야 한다.</li>
			<li>데이터의 밸런싱<br/>
스토리지 서비스를 중단하지 않고 node의 개수를 늘렸을 때 저장된 데이터가 아래의 조건을 만족하는 상태로 리밸런싱 된다. 리벨런싱할 때 데이터양이 최대인 node의 데이터양이 최소인 node의 데이터양의 두 배를 넘지 않는다.</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#5-approach" id="user-content-5-approach"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>5. Approach</h2>
<ul>
	<li>분산 환경<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/831/%EA%B7%B8%EB%A6%BC3.jpg" src="https://camo.githubusercontent.com/4552bf33e1aa5932626afa2119df1e6b593d0c56/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3833312f254541254237254238254542254136254243332e6a7067"/>
	<ul>
		<li>Apache ZooKeeper를 이용하여 분산된 다수의 Node를 관리한다. 이렇게 구성된 Node들에 대량의 key-value 데이터를 균형 있게 저장한다. ZooKeeper는 각 노드의 변경 사항을 관리하고 노드들끼리 싱크를 맞춰준다. ZooKeeper는 데이터를 디렉터리 구조로 저장하고, 데이터가 변경되면 클라이언트에게 어떤 노드가 변경됐는지 콜백을 통해서 알려준다. 데이터를 저장할 때 해당 세션이 유효한 동안 데이터가 저장되는 Ephemeral Node라는 것이 존재하고, 데이터를 저장하는 순서에 따라 자동으로 일련번호(sequence number)가 붙는 Sequence Node라는 것도 존재한다. 조금 과장하면 이러한 기능이 ZooKeeper 기능의 전부다. 이런 심플한 기능을 가지고 자신의 입맛에 맞게 확장해서 사용하면 된다.</li>
		<li>ZooKeeper callback<br/>
ZooKeeper의 여러 기능 가운데 가장 활용도가 높은 부분이 바로 데이터 변경을 감시하여 콜백을 실행하는 감시자(Watcher)다. 감시자를 트리 내의 특정 노드에 등록하면 별도로 데이터 폴링을 하지 않더라도 노드 변경 사항을 전달받을 수 있다. 따라서 서비스에 동적으로 설정을 반영하고자 할 때 ZooKeeper를 많이 사용한다. 첫 번째로 Persistent Node는 한 번 저장되고 나면 세션이 종료되어도 삭제되지 않고 유지되는 노드다. 즉, 명시적으로 삭제하지 않는 한 해당 데이터는 삭제 및 변경되지 않는다. 두 번째로 Ephemeral Node는 특정 노드를 생성한 세션이 유효한 동안 그 노드의 데이터가 유효한 노드다. 좀 더 자세히 설명하면 ZooKeeper Server에 접속한 클라이언트가 특정 노드를 Ephermeral Node로 생성했다면 그 클라이언트와 서버 간의 세션이 끊어지면, 즉 클라이언트와 서버 간의 Ping을 제대로 처리하지 못한다면 해당 노드는 자동으로 삭제된다. 이 기능을 통해 클라이언트가 동작하는지 여부를 쉽게 판단할 수 있다. 따라서 추가적으로 각 노드의 변경 사항을 관리하고 노드들끼리 싱크를 맞추는 것은 직접 구현할 필요가 없다.</li>
		<li>분산환경은 Read/Write scalability를 확보한다.<br/>
Read/Write load can be balanced across multiple nodes.</li>
		<li>Overlord와 Node의 분산환경은 높은 availability를 확보한다.<br/>
Overlord 여러개, Node 여러개, 동시에 죽지 않으면 접근 가능하기 때문이다. Isolated parts of the database can serve read/write requests in case of network partition.</li>
	</ul>
</li>
	<li>실시간<br/>
Consistent hashing 알고리즘을 사용하여 서비스를 중단하지 않고 Node를 추가/제거할 수 있게 하였고, key-value 데이터 균형을 유지한다. Consistent hashing은 하나의 노드가 일정 범위의 해시값을 가지는 데이터들을 각각 처리하도록 하는 모델이다. 이 모델에서 노드가 추가되면 인접한 다른 노드에서 적절한 양의 데이터를 할당받고, 노드가 제거되면 마찬가지로 인접한 다른 노드들이 제거된 노드의 데이터를 적절히 나눠 할당받기 때문에 다른 노드들은 영향을 받지 않는다는 장점이 있다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/830/%EA%B7%B8%EB%A6%BC1.png" src="https://camo.githubusercontent.com/0a37c6140d6bcdb5cb90654f990534d9a796b41f/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3833302f254541254237254238254542254136254243312e706e67"/>
</li>
	<li>Consistent<br/>
Consistency is a much more complicated property than the previous ones, so we have to discuss different options in detail. It beyond this article to go deeply into theoretical consistency and concurrency models, so we use a very lean framework of simple properties.
	<ul>
		<li>Write는 Master에만 쓰기 때문에 기본적인 consistency가 보장된다.<br/>
단, Slave Node는 consistency를 보장하지 않기 때문에 weak read를 Master Node는 strong read를 제공한다.<br/>
Read-Write consistency. From the read-write perspective, the basic goal of a database is to minimize a replica convergence time (how long does it take to propagate an update to all replicas) and guarantee eventual consistency. Besides these weak guarantees, one can be interested in stronger consistency properties:</li>
		<li>Overlord를 여러개 쓸 때 consistency가 보장하지 않을 가능성이 있다.<br/>
Consistency-scalability tradeoff. One can see that even read-write consistency guarantees impose serious limitations on a replica set scalability, and write-write conflicts can be handled in a relatively scalable fashion only in the Atomic Writes model. The Atomic Read-modify-write model introduces short casual dependencies between data and this immediately requires global locking to prevent conflicts. This shows that even a slight spatial or casual dependency between data entries or operations could kill scalability, so separation of data into independent shards and careful data modeling is extremely important for scalability</li>
	</ul>
</li>
	<li>안전한 저장소<br/>
Data persistence. Node failures within certain limits do not cause data loss.
	<ul>
		<li>Overlord의 durability<br/>
ZooKeeper는 강한 durability를 보장한다. ZooKeeper는 data 그 자체는 언제나 메모리에만 존재하며 서버의 로컬디스크에 트랜잭션 로그와 스냅 샷을 저장하기 때문에 메모리가 날아가도 트랜잭션 로그와 스냅 샷을 이용하여 data를 복구할 수 있기 때문이다.<br/>
안전한 저장소를 구현하기 위해 Master/slave model을 도입하여 데이터 사본을 구현하였다. 일부 노드가 정상 작동하지 않아도 전체 시스템이 정상적으로 작동되어야 하므로 노드마다 해당 노드의 데이터를 백업하고, 복원할 방법이 있어야 하기 때문이다. 보통은 Master가 Overlords와 통신을 하고 slave는 Master의 데이터를 주기적으로 백업하여 저장하도록 했다. Master가 정상작동하지 않으면 Overlords는 slaves 중에 하나를 선택하여 Master의 임무를 수행하도록 한다.</li>
		<li>Slave replication<br/>
본 프로젝트의 저장소 시스템은 일부 노드가 정상 작동하지 않아도 전체 시스템이 정상적으로 작동되어야 하기 때문에 각 노드마다 해당 노드의 데이터를 백업하고, 복원할 수 있는 방법이 있어야 한다. 때문에 각 Node에 Master-Slave 모델을 적용하여 보통은 Master가 Overlords와 통신을 하고 slave는 Master의 데이터를 주기적으로 백업하여 저장하도록 했다. Master가 정상작동 하지 않으면 Overlords는 slaves 중에 하나를 선택하여 Master로서의 역할을 수행하도록 한다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/829/%EA%B7%B8%EB%A6%BC2.png" src="https://camo.githubusercontent.com/6b4f14f739e46e2f9d677505de2e745ba2a3b5d0/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3832392f254541254237254238254542254136254243322e706e67"/>
		<ul>
			<li>Data Consistency<br/>
It is well known and fairly obvious that in geographically distributed systems or other environments with probable network partitions or delays it is not generally possible to maintain high availability without sacrificing consistency because isolated parts of the database have to operate independently in case of network partition. This fact is often referred to as the <span>CAP</span> theorem. However, consistency is a very expensive thing in distributed systems, so it can be traded not only to availability. It is often involved into multiple tradeoffs. To study these tradeoffs, we first note that consistency issues in distributed systems are induced by the replication and the spatial separation of coupled data, so we have to start with goals and desired properties of the replication:</li>
			<li>Fault-tolerance.<br/>
Ability to serve read/write requests does not depend on availability of any particular node.</li>
		</ul>
</li>
	</ul>
</li>
	<li>이용 편의성<br/>
저장소에 접근하기 위한 클라이언트 라이브러리를 구현하였고 이 라이브러리를 이용하여 간단한 클라이언트와 모니터링 클라이언트를 구현하였다.
	<ul>
		<li>클라이언트 다양성<br/>
모든 통신은 <span>JSON</span> 형식으로 전달된다. 정확한 <span>JSON</span> 형식만 맞으면 된다. <span>JSON</span> 형식이기 때문에, key-value 가 나오는 순서를 굳이 맞출 필요는 없다. JSON을 이용해 value를 저장하기 때문에 다양한 형태의 value를 지원한다.</li>
		<li>모니터링<br/>
웹을 통해 Overlord와 Node의 상태확인이 가능하다.</li>
		<li>Web test client<br/>
웹을 통해 key-value get, set을 할 수 있다.</li>
	</ul>
</li>
	<li>통신 계층<br/>
Overlord와 Node 간의 네트워크 통신, Node와 Node 간의 네트워크 통신, Overlord와 Client 간의 네트워크 통신이 필요하다. 네트워크 통신을 위한 저수준의 통신을 직접 구현하지 않고 ZeroMQ라는 Message Queue를 사용해 통신 구현을 쉽게 할 수 있도록 한다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/832/%EA%B7%B8%EB%A6%BC4.jpg" src="https://camo.githubusercontent.com/516ac417599d457ec0bf45df876a3cb15e003c4f/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3833322f254541254237254238254542254136254243342e6a7067"/>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#6-project-architecture" id="user-content-6-project-architecture"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>6. Project Architecture</h2>
<ul>
	<li>Architecture Diagram<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/834/%EA%B7%B8%EB%A6%BC5.png" src="https://camo.githubusercontent.com/e4862f39c0719d7d7501244e24f5d029a26feb46/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3833342f254541254237254238254542254136254243352e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/840/%EA%B7%B8%EB%A6%BC11.png" src="https://camo.githubusercontent.com/e4f4055026342a3e4e9c7b218dace5a1f79e391c/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834302f25454125423725423825454225413625424331312e706e67"/>
</li>
	<li>Architecture Description
	<ul>
		<li>DiMint Client<br/>
Overlord의 IP를 알고 있으며 해당 IP로 library 에서 제공하는 커맨드를 이용하여 DiMint Server System과 통신한다. DiMint Server에 관해서 IP 이외의 정보를 알 필요가 없기 때문에 사용하기 편리하다..</li>
		<li>DiMint Server System
		<ul>
			<li>Overlords<br/>
세개의 서버로 구성되어 있으며 DiMint Client에서 보내는 모든 커맨드를 처리한다. 세개인 이유는 Overlords가 시스템의 핵심이기 때문에 서버가 세개인 경우 한 개의 서버가 정상 작동 하지 않아도 전체 시스템은 정상적으로 작동할 수 있기 때문이다. Overlords는 서비스를 요청한 Client의 정보를 가지고 consistent hashing을 하여 어떤 노드에 access하여 명령을 처리할 지 결정한다. 또한 항상 Node들의 부하를 체크하여 모니터링을 할 수 있도록 하며 관리자가 노드를 추가하거나 제거하는 경우에 필요한 처리들을 수행한다.
			<ul>
				<li>Zookeeper<br/>
일반적으로 3대 이상을 사용하며 서버 수는 주로 홀수로 구성한다. 서버 간의 데이터 불일치가 발생하면 데이터 보정이 필요한데 이때 과반수의 룰을 적용하기 때문에 서버를 홀수로 구성하는 것이 데이터 적합성 측면에서 유리하다. Zookeeper 서버는 Leader와 Follower로 구성되어 있다. 서버들끼리 자동으로 Leader를 선정하며 모든 데이터 저장을 주도한다.
				<ul>
					<li>ZooKeeper 메모리<br/>
ZooKeeper는 데이터를 디스크에 영구 저장하긴 하지만, 빠른 처리를 위해 모든 트리 노드를 메모리에 올려놓고 처리한다. 즉 대규모의 데이터를 처리하기엔 무리가 있다. 따라서 Memcached나 Redis와 같은 캐시 서버와는 다른 용도로 쓰인다.</li>
					<li>ZooKeeper 앙상블<br/>
분산 작업을 제어하기 위해 사용되는 만큼 ZooKeeper 서버가 중단되면 ZooKeeper에 의존하는 모든 서버가 영향을 받는다. 따라서 ZooKeeper는 최대한 정상 동작을 보장하여야 한다. 이를 위해 여러 대의 ZooKeeper 서버를 클러스터링하여 고가용성을 지원하도록 설정할 수 있다. 이 ZooKeeper 클러스터를 앙상블(Ensemble)이라 부른다.<br/>
앙상블로 묶인 ZooKeeper 서버 중 한 대는 쓰기 명령을 총괄하는 리더 역할을 수행하고, 나머지는 팔로어 역할을 수행한다. 클라이언트에서 ZooKeeper 앙상블에 연결할 때는 커넥션 문자열에 앙상블을 구성하는 ZooKeeper 서버 주소를 다수 포함할 수 있다. 클라이언트는 커넥션 문자열에 포함된 ZooKeeper 주소 중 하나에 접근하여 세션을 맺는다. 클라이언트가 전달한 읽기 명령은 현재 연결된 ZooKeeper 서버에서 바로 반환한다. 이에 비해 쓰기 명령은 앙상블 중 리더 역할을 수행하는 ZooKeeper 서버로 전달되며, 리더 ZooKeeper는 모든 팔로어 ZooKeeper에게 해당 쓰기를 수행할 수 있는지 질의한다. 만약 팔로어 중 과반수(&gt; n/2)의 팔로어로부터 쓸 수 있다는 응답을 받으면 리더는 팔로어에게 데이터를 쓰도록 지시한다.<br/>
모든 ZooKeeper 서버 주소를 클라이언트로 지정해 놓았다면 현재 연결한 ZooKeeper 서버가 중단되더라도 자동으로 다른 ZooKeeper 서버에 세션을 유지한 채로 다시 연결한다. 이와 같은 특성으로 인해 앙상블을 구성하는 ZooKeeper 서버 중 과반수의 서버가 현재 동작한다면 데이터 읽기 쓰기를 정상적으로 처리한다. 즉, ZooKeeper 서버 3대로 앙상블을 구성하였으면 ZooKeeper 한 대가 중단되더라도 문제 없다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/844/%EA%B7%B8%EB%A6%BC6.png" src="https://camo.githubusercontent.com/37ffed63df2ea126104f8b3e4488f387a754bfd0/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834342f254541254237254238254542254136254243362e706e67"/>
</li>
				</ul>
</li>
				<li>Consistent hashing<br/>
Overlord에서 key를 sharding 할 때 사용한다.<br/>
Node가 동적으로 추가/제거되었을 때 모든 노드에 대해 key-value pair rebalancing 을 하지 않기 때문에 효율적이다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/845/%EA%B7%B8%EB%A6%BC7.png" src="https://camo.githubusercontent.com/a9f47010b2b1382c96ef4eaa97b3cd9d912ceb6b/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834352f254541254237254238254542254136254243372e706e67"/>
</li>
				<li>ClientConnector<br/>
Client의 접속을 받아들이는 모듈이다. 모든 Client의 명령은 여기서부터 시작되며, 명령은 NodeInfoManager에 전달된다. 또한 NodeInfoManager로부터 결과를 받아 Client에게 결과를 전송한다.</li>
				<li>NodeInfoManager<br/>
Overlord에 현재 연결되어 있는 Node의 정보를 저장하고 있는 모듈. HealthChecker 모듈을 통해 얻은 Node들의 현재 용량 상태, Node별 키 배분 현황, Node의 Master/Slave 상태, 각 키의 용량 등의 정보를 저장하고, 가지고 있는 정보를 제공하는 API를 가지고 있다. ClientConnector를 통해 get/set key 명령이 들어올 경우, 이 모듈을 통해 CommandSender에 명령을 전달한다.</li>
				<li>HealthChecker<br/>
각 node에 node의 상태를 요청하는 API를 전송하고, 결과를 받아 NodeInfoManager에 전송하는 모듈.</li>
				<li>CommandCenter<br/>
node에게 명령을 전송하고, 결과를 받는 모듈. 이 명령에서는 master node로 임명, rebalancing, get by key, set key value 등이 포함된다.</li>
				<li>NodeConnector<br/>
노드와의 연결을 관장하는 모듈. 새 node의 추가는 이 모듈을 통해 들어오며, NodeConnector는 NodeInfoManager에게 새 Node가 추가되었음을 알린다.</li>
				<li>Rebalancer<br/>
리밸런싱에 관여하는 모듈. 데이터 리밸런싱을 해야 하는 상황이 오면(노드의 추가, 제거) NodeInfoManager에서 현재 node들의 상황을 받아온 뒤, 어떤 node가 무슨 key를 어떤 node에 전송해야 할지 결정하고 CommandSender에 전송한다. 그 후 NodeInfoManager에 알려 Node의 key 배분 상태를 업데이트하게 한다.</li>
				<li>Master/Slave Manager<br/>
노드의 Master/Slave 관련 명령을 관장하는 모듈. Master Node 중 하나가 dead 상태가 되었을 경우, 해당 Master에 속해 있던 Slave Node 중 하나를 Master Node로 승격시키는 명령을 CommandCenter에 전송한다. 그리고 나서 NodeInfoManager에 해당 사실을 알려 Node 상태를 업데이트하게 한다.</li>
			</ul>
</li>
			<li>Nodes<br/>
실제적인 데이터 저장소이다. 각 노드는 Overlords에 직접적으로 연결되어있고, 필요한 경우 Overlord에서 다른 노드의 정보를 받아와서 노드들끼리 서로 통신하여 data rebalancing을 수행한다.
			<ul>
				<li>OverlordConnector<br/>
overlord와의 접속에 관여하는 모듈. 처음 node process가 생성되면 overlord에 접속을 시도한다.</li>
				<li>CommandCenter<br/>
overlord의 CommandSender서 보내진 명령을 받고, 적절한 응답을 전달하는 모듈로, 여기서 어떤 명령인지 해석한 이후 Storage, Rebalancer, RoleManager, Recovery, HealthReporter 모듈 등에 명령이 전달된다.</li>
				<li>HealthReporter<br/>
Overlord의 HealthChecker가 node의 상태를 요청할 때, 자신의 상태를 전달하는 모듈. Stoarge 모듈로 가서 해당 node가 사용하는 memory 사용량, 각 key가 차지하는 용량 등을 overlord에 전송한다.</li>
				<li>Storage<br/>
실제로 Key-Value가 저장되어 있는 dictionary가 포함되어 있는 모듈. 단순히 key-value 저장/key에 대한 value값 리턴 이외에도 자신이 가지고 있는 key의 갯수 및 용량 정보를 가지고 있고 이를 리턴하는 API를 갖추고 있다.</li>
				<li>RoleManager<br/>
해당 node가 master node인 경우, Storage에 값이 저장되거나 변경될 때, Storage가 변경점을 RoleManager에게 전송하고 RoleManager는 자신의 slave node에 변경사항을 전송한다. 해당 node가 slave node인 경우, master node로부터 오는 log 정보를 받아 자신의 정보를 업데이트하기 위해 Stoarge 모듈에 해당 정보를 전달한다.</li>
				<li>Rebalancer<br/>
해당 node의 consistent hashing key가 변경되어 다른 node로 데이터를 옮겨야 할 경우, 다른 node의 주소와 옮겨야 할 키 목록을 overlord로부터 받아, 대상 node의 Rebalancer 모듈에 key-value dictionary를 전송하고, 원본 노드에서는 삭제한다. 반면, key-value dict를 받은 node 쪽에선 해당 dictionary를 Storage에 전달해서 key-value 정보를 업데이트한다</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#7-implementation-spec" id="user-content-7-implementation-spec"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>7. Implementation Spec</h2>
<ul>
	<li>Input/Output Interface
	<ul>
		<li>Client<br/>
DiMinit Server의 remote storage를 이용하기 위한 인터페이스들을 제공한다.
		<ul>
			<li>get(key)<br/>
Server의 storage에서 input인 key에 mapping되어 저장된 value를 가져오기 위한 메서드이다. key는 내부에서 string type으로 변환되어 사용된다. 실행 결과값은 storage에 key에 mapping된 value가 존재하면 해당 value를 반환하며, 그렇지 않으면 Error message를 반환한다.</li>
			<li>set(key, value)<br/>
Server의 storage에 key와 value를 mapping하여 저장한다. key type이 string type으로 변환되어 사용되는 데에 반해, value type은 null, int, boolean, string, list, dictionary 등의 type을 지원한다. 정상적으로 저장이 되었을 경우 해당 value를 반환하며, 저장에 실패했을 경우 Error message를 반환한다.</li>
		</ul>
</li>
	</ul>
</li>
	<li>Inter Module Communication Interface<br/>
모든 통신은 <acronym title="JavaScript Object Notation"><span>JSON</span></acronym> 형식으로 전달된다. 따라서 정확한 <span>JSON</span> 형식이어야 하며, 그렇지 않으면 에러 처리되어야 한다. <span>JSON</span> 형식이기 때문에, key-value 가 나오는 순서를 굳이 맞출 필요는 없다. 모든 요청은 대소문자를 구분한다.
	<ul>
		<li>Key-value get, set를 할 때
		<ul>
			<li>Key-value get, set를 할 때 Client, Overlord, Node간 통신<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/839/%EA%B7%B8%EB%A6%BC12.png" src="https://camo.githubusercontent.com/5d97567574ad42a30503256a5d1c001dfe126fc1/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3833392f25454125423725423825454225413625424331322e706e67"/>
</li>
			<li>Key-value get, set를 할 때 Client, Overlord, Node간 Interface<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/847/%EA%B7%B8%EB%A6%BC9.png" src="https://camo.githubusercontent.com/5544ec2ffa4523e70b5b1a587942d7e62e1f2ae8/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834372f254541254237254238254542254136254243392e706e67"/>
</li>
		</ul>
</li>
		<li>Node를 추가/제거할 때
		<ul>
			<li>Node를 추가/제거할 때 Overlord, Node간 통신<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/841/%EA%B7%B8%EB%A6%BC13.png" src="https://camo.githubusercontent.com/0dbee7638b938919a23106015ba4d0447073e0b2/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834312f25454125423725423825454225413625424331332e706e67"/>
</li>
			<li>Node를 추가/제거할 때 Overlord, Node간 Interface<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/846/%EA%B7%B8%EB%A6%BC8.png" src="https://camo.githubusercontent.com/5a1a30c09914aed2ffc2209c8105911e46fd24a2/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834362f254541254237254238254542254136254243382e706e67"/>
</li>
		</ul>
</li>
		<li>Health check 및 data rebalancing
		<ul>
			<li>Health check할 때 Overlord, Node간 통신<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/842/%EA%B7%B8%EB%A6%BC14.png" src="https://camo.githubusercontent.com/5836474b3463fae1cad91511b708e427491d94fe/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834322f25454125423725423825454225413625424331342e706e67"/>
</li>
			<li>Data rebalancing할 때 Overlord, Node간 통신<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/843/%EA%B7%B8%EB%A6%BC15.png" src="https://camo.githubusercontent.com/c78d87712b13a23c9396d708b2d7a3bb70a72977/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834332f25454125423725423825454225413625424331352e706e67"/>
</li>
			<li>Health check 및 data rebalancing할 때 Overlord, Node간 interface<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/848/%EA%B7%B8%EB%A6%BC10.png" src="https://camo.githubusercontent.com/f721f29c8ecdcfbe02095906d806d78f7f30adf2/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f3834382f25454125423725423825454225413625424331302e706e67"/>
</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<ul>
	<li>
<span>API</span> Lists
	<ul>
		<li>Common
		<ul>
			<li>Error Response https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Error-Response</li>
		</ul>
</li>
		<li>Client
		<ul>
			<li>Get Overlords https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@Get-Overlords</li>
			<li>Get https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@Get</li>
			<li>Set https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@Set</li>
			<li>Incr https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@Incr</li>
			<li>Decr https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@Incr</li>
			<li>State https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Client@State</li>
			<li>Overlord_State https://github.com/DiMint/DiMint_Overlord/wiki/DiMint%20Network%20Protocol@Client@Overlord_State</li>
		</ul>
</li>
		<li>Overlord
		<ul>
			<li>Get https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Get</li>
			<li>Set https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Set</li>
			<li>Incr https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Incr</li>
			<li>Decr https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Decr</li>
			<li>Move Keys https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Move-Key</li>
			<li>Nominate Master https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Nominate-Master</li>
			<li>Add Slave https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Add-Slave</li>
			<li>Delete Slave https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Overlord@Delete-Slave</li>
		</ul>
</li>
		<li>Node
		<ul>
			<li>To Overlord
			<ul>
				<li>Connect https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Node@To-Overlord@Connect</li>
			</ul>
</li>
			<li>To Node
			<ul>
				<li>To Slave
				<ul>
					<li>Log https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Node@To-Slave@Log</li>
					<li>Dump https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Node@To-Slave@Dump</li>
				</ul>
</li>
				<li>To Other Node
				<ul>
					<li>Transfer https://github.com/DiMint/DiMint_Overlord/wiki/DiMint-Network-Protocol@Node@To-Other-Node@Transfer</li>
				</ul>
</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<ul>
	<li>Modules
	<ul>
		<li>Client<br/>
DiMinit Server의 remote storage를 이용하기 위한 인터페이스들을 제공한다.
		<ul>
			<li>OverlordConnector<br/>
Overlord와의 첫 연결에 관여하는 모듈이다.</li>
			<li>CommandCenter<br/>
Overlord에 각종 명령을 보내고, 그 결과를 받을 때 이용하는 모듈이다. get, set, monitor 명령이 이에 해당한다.</li>
		</ul>
</li>
		<li>Overlord<br/>
Client interface에서 제공하는 요청들을 처리하고, Node들을 관리한다. 앞에서 설명한 아키텍쳐에서 아래 모듈들이 어떤 아키텍쳐 구현을 하는가
		<ul>
			<li>Hash<br/>
Rebalancer를 구현한다.</li>
			<li>Network<br/>
Client Connector와 Node Connector를 구현한다.</li>
			<li>OverlordStateTask<br/>
Health Checker를 구현한다.</li>
			<li>ZooKeeperManager<br/>
Node Info Manager, Master/Slave Manager, Health Checker를 구현한다.</li>
			<li>OverlordTask<br/>
Client Connector와 Node Connector를 구현한다.</li>
			<li>OverlordRebalanceTask<br/>
Node Info Manager와 Rebalancer를 구현한다.</li>
			<li>handler<br/>
CommandCenter에서 Node 종료 명령어를 처리한다</li>
		</ul>
</li>
		<li>Node<br/>
DiMint는 key-value 데이터를 메모리에 저장하여 관리한다.	단, 메모리는 Node가 다운되었을 때 데이터가 유실되는 문제가 있기 때문에 Master-Slave model로 구성하기로 했다.
		<ul>
			<li>ZooKeeperManager<br/>
Health Reporter를 구현한다.</li>
			<li>NodeTransferTask<br/>
Command Center, Role Manager, Rebalancer를 구현한다.</li>
			<li>NodeStateTask<br/>
Health Reporter를 구현한다.</li>
			<li>NodeReceiveMasterTask<br/>
Command Center, Role Manager, Rebalancer를 구현한다.</li>
			<li>NodeIntervalDumpTask<br/>
Command Center, Role Manager, Rebalancer를 구현한다.</li>
			<li>Node<br/>
Overlord Connector를 구현한다.</li>
		</ul>
</li>
		<li>ZooKeeper 디렉터리 구조<br/>
위의 요구 사항을 만족시키기 위해 ZooKeeper의 분산 저장소 기능을 사용해 디렉터리 구조를 설계했다.<br/>
<pre><br/>
/dimint<br/>
	/overlord<br/>
		/host_list<br/>
			overlord_1<br/>
			overlord_2<br/>
			overlord_3<br/>
	/node<br/>
		/list<br/>
			master_1<br/>
			slave_1_1<br/>
			slave_1_2<br/>
			master_2<br/>
			slave_2_1<br/>
			master_3<br/>
			slave_3_1<br/>
			slave_3_2<br/>
			slave_3_3<br/>
		/role<br/>
			/master_1<br/>
				slave_1_1<br/>
				slave_1_2<br/>
			/master_2<br/>
				slave_2_1<br/>
			/master_3<br/>
				slave_3_1<br/>
				slave_3_2<br/>
				slave_3_3<br/>
</pre>
		<ul>
			<li>/dimint/overlord에는 Overlord들을 생성한다.</li>
			<li>/dimint/node/list에는 살아있는 모든 Node들이 생성되어 있다.<br/>
해당 Node에는 Node state가 저장된다.</li>
			<li>/dimint/node/role에는 Master Node와 Slave Node가 구분되어 저장되어 있다.<br/>
Slave Node는 자신의 Master Node 하위에 위치한다.</li>
		</ul>
</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#8-solution" id="user-content-8-solution"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>8. Solution</h2>
<ol>
	<li>Implementation Details
	<ul>
		<li>Client
		<ul>
			<li>DiMintClient<br/>
DiMint Storage를 이용하려는 사람들이 등록하고 이용하는, 라이브러리 형태의 모듈. 사용자들은 DiMintClient 외의 class를 건드릴 필요가 없다.
			<ul>
				<li>__connect<br/>
Overlord에 연결하는 메서드로, DiMintClient에서는 Connection 객체를 새로 생성한다. 실제로 Overlord와의 통신은 Connection이 담당한다.</li>
				<li>get<br/>
remote storage로부터 특정 key에 해당하는 값을 가져온다.</li>
				<li>get_strong<br/>
통상적인 get은 slave에서 값을 가져오는데 비해, get_strong은 master로부터 값을 가져온다.</li>
				<li>set<br/>
remote storage에 특정 키와 값을 전달하여, 저장한다.</li>
				<li>incr<br/>
remote storage에 있는 특정 키의 값을 1 증가시킨다.</li>
				<li>decr<br/>
remote storage에 있는 특정 키의 값을 1 감소시킨다.</li>
				<li>del<br/>
remote storage에 있는 특정 키의 내용을 삭제한다.</li>
				<li>state<br/>
현재 Overlord에 연결되어 있는 모든 Node의 상태를 가져온다.</li>
				<li>overlord_state<br/>
현재 이용 가능한 Overlord의 상태를 가져온다.</li>
			</ul>
</li>
			<li>Connection<br/>
Overlord와의 연결을 담당하는 모듈
			<ul>
				<li>__connect<br/>
zmq를 이용하여 소켓을 열고, Overlord 중 하나에 접속한 뒤 Overlord 리스트를 요청한다. 그 이후 모든 Overlord에 소켓을 만들어 균등하게 요청을 보내도록 한다.</li>
				<li>get, get_strong, set, state, overlord_state, incr, decr, del<br/>
DiMintClient 부분과 하는 일은 같다. 여기서는 Command 클래스를 통해 명령을 생성한 뒤, <i>send를 통해 Overlord에 전송한다.<br/>
#<strong>*</strong></i>send<br/>
여러 개의 Overlord 중 임의의 한 Overlord를 선택한다. 그 후 인자로 받은 command를 선택된 overlord에 zmq로 전송한다. 그리고 overlord의 응답을 반환한다.</li>
				<li>__get_overlord_location<br/>
__send 명령을 이용하여 Overlord에 요청을 보내려 할 때, 어느 Overlord에 요청할 지 전달한다.</li>
			</ul>
</li>
			<li>Command<br/>
Overlord에 전달할 명령을 생성하는 클래스이다.
			<ul>
				<li>get_by_key<br/>
key 인자를 받고, get에 해당하는 명령을 생성한다. 만약 is_strong 인자가 True로 들어오면 consistency를 강화한 get(master node에 get을 요청)을 위한 command를 생성한다.</li>
				<li>set_value<br/>
key와 value를 받아, storage에 해당 값을 등록하는 명령을 생성한다.</li>
				<li>get_state<br/>
각 노드의 상태를 확인할 수 있는 명령을 생성한다.</li>
				<li>get_overlord_state<br/>
각 overlord의 상태를 확인할 수 있는 명령을 생성한다.</li>
				<li>__validate_keys<br/>
key가 필요한 명령을 만들어야 할 때, 키가 맞는 것인지 검사한다. 올바른 key는 str 형태여야 한다.</li>
				<li>__validate_value<br/>
value가 필요한 명령을 만들어야 할 때, value가 형식에 맞는지 확인한다. 올바른 value는 json 형식을 따른다</li>
			</ul>
</li>
		</ul>
</li>
		<li>Overlord<br/>
Overlord 모듈은 Zookeeper와 밀접한 관련을 맺고 있으며, Client의 명령을 받아 Node에게 전달하고, Node의 응답을 받아 Client에게 해당 응답을 전달하는 역할을 한다. 또한 Node의 추가 및 제거, 데이터 리밸런싱 또한 담당한다.
		<ul>
			<li>Hash
			<ul>
				<li>get_node_id<br/>
Node가 처음 overlord에 접속했을 때 Node에게 임의의 값을 부여해, 고유하게 쓸 수 있게 한다.</li>
			</ul>
</li>
			<li>Network
			<ul>
				<li>get_ip<br/>
overlord가 실행되고 있는 서버의 ip 주소를 가져온다.</li>
			</ul>
</li>
			<li>OverlordStateTask
			<ul>
				<li>run<br/>
10초마다, zookeeper에 있는 overlord의 상태를 기록하는 저장 공간에 자신의 현재 상태(cpu, 메모리 사용량 등)를 기록한다.</li>
			</ul>
</li>
			<li>ZooKeeperManager<br/>
zookeeper와 직접적으로 접근하며, zookeeper에 정보를 저장, 수정, 삭제하는 역할을 담당한다.
			<ul>
				<li>get_node_list<br/>
현재 zookeeper에 접속 중인 모든 node id list를 반환한다.</li>
				<li>get_master_info_list<br/>
현재 등록되어 있는 Node 중 master 역할을 수행하고 있는 node의 id만을 반환한다.</li>
				<li>delete_node<br/>
Node 삭제 요청을 받았을 때 삭제 프로세스를 수행한다. 만약 삭제하려는 노드가 slave이면 그냥 node의 정보를 삭제하면 된다. 그러나 master일 경우 많은 일을 수행한다. 자세한 일은 kill_node가 수행한다.</li>
				<li>get_identity<br/>
node에 실제로 부여할 node id를 반환한다.</li>
				<li>determine_node_role<br/>
새 node가 추가되었을 때, 해당 노드가 master일지 slave일지, slave라면 어떤 master의 slave가 되어야 할 지 결정한다. master가 모두 정량의 slave로 채워져 있으면 새 node는 master가 되고, 그렇지 않으면 slave가 부족한 master node의 slave node가 된다.</li>
				<li>select_node<br/>
특정 key에 대해, 해당 key를 가져오거나 쓸 node를 결정한다. key를 쓸 때는 당연히 해당 key를 가지고 있는 master node를 선택하고, get 명령일 때는 slave들 중 하나를 선택한다. strong consistency get의 경우 master를 선택한다.</li>
				<li>__select_master_node<br/>
특정 key가 들어왔을 때, 해당 key가 들어있는 master node를 찾는다. 각 master node는 고유한 value를 가지고 있고 key도 적절한 과정을 통해 value를 가지고 그 value를 바탕으로 어느 master node에 저장될지 선택하게 된다. 따라서 특정 key가 어떤 master node에 저장되어 있는지 알 수 있다.</li>
				<li>__set_master_node_attr<br/>
master node가 가지고 있는 고유한 특성(consistent hashing에 쓰이는 value 등)을 저장한다.</li>
				<li>add_key_to_node<br/>
overlord에서 어떤 key가 어떤 master에 저장되었고 master가 key를 얼마나 가졌는지 알기 위해 해당 정보를 저장한다.</li>
				<li>remove_key_list_from_node<br/>
add_key_to_node의 반대. 특정 master node에 들어있던 key를 삭제할 일이 생겼을 때 zookeeper에 그 내용을 삭제한다.</li>
				<li>check_dead_node_list<br/>
현재 활성화된 node list가 저장되어 있는 폴더의 watcher이다. 이전에 저장되었던 node list보다 현재의 node list가 적을 경우 특정 node가 삭제된 것이므로 적당한 조치를 취하게 한다.</li>
				<li>kill_node<br/>
node가 삭제되었을 경우(유저에 의한 삭제든 system failure에 의한 삭제든) 해야 할 조치를 수행한다. slave가 있으면 kill_node에서 정의된 프로세스를 바로 수행하면 되지만, 만약 slave가 없다면 자신이 현재 가지고 있는 key를 다른 node에 모두 옮겨야 한다.</li>
				<li>get_node_info<br/>
특정 id의 node에 대한 정보를 반환한다. 이 정보에는 각종 정보를 주고받기 위한 주소와 포트, consistent hashing value들이 저장되어 있다. master node일 경우, 해당 node가 가지고 있는 slave node 또한 list로 만들어 반환한다.</li>
			</ul>
</li>
			<li>OverlordTask<br/>
주된 역할을 수행하는 클래스이다. client의 요청을 직접 받으며, node에게 명령을 전달한다. 반대로 node에게서 응답을 받고 client에게 요청에 대한 응답을 전송한다.
			<ul>
				<li>__process_request<br/>
client의 요청을 파싱하고 적절한 명령을 수행한다. get, set, state, overlord_state에 대한 처리를 담당한다.</li>
				<li>__process_repsonse<br/>
node에서 온 응답을 파싱하고 적절한 명령을 수행한다. node로부터의 접속, client의 요청을 node에 전송했을 때(get, set 등) 받은 응답을 다시 client에 처리하는 역할을 담당한다.</li>
				<li>add_node<br/>
새 node가 추가되었을 때 이 메서드를 수행한다. node의 역할을 결정하고, 만약 새로운 master node로 임명되어 리밸런싱이 필요해질 경우 리밸런싱을 요청한다.</li>
				<li>__get_rebalnace_info<br/>
리밸런싱이 필요할 경우 어떤 node에 있는 어떤 key들을 어떤 node에 전달할 것인지 결정하여 새로운 요청을 전송한다.</li>
			</ul>
</li>
			<li>OverlordRebalanceTask<br/>
리밸런싱을 실제로 수행하는 쓰레드이다. 10초에 한번씩 node에 저장된 키의 갯수를 비교하여 리밸런싱이 필요하다고 판단되면 node의 고유value값이 인접한 적절한 두 노드를 선택하여 두 노드간에 key-value data를 주고 받을 수 있게 한다.
			<ul>
				<li>__select_nodes_ids<br/>
리밸런싱의 대상이 될 인접한 두 노드를 선택한다.</li>
				<li>select_move_keys<br/>
리밸런싱의 대상이 될 노드들로부터 어떤 키를 이동시킬 것인지를 결정한다.</li>
			</ul>
</li>
			<li>handler<br/>
stop 명령을 받으면 오버로드 프로세스를 중단한다.</li>
			<li>Overlord<br/>
오버로드를 띄우는데 필요한 설정들을 로드하고, 수행해야 할 태스크들의 인스턴스를 만든 후 태스크를 각각 실행시킨다.</li>
		</ul>
</li>
		<li>Node<br/>
Node 모듈은 Overlord에서 명령을 받아 해당 명령을 storage에서 수행 한 후 Overlord에게 다시 결과값을 반환해준다.
		<ul>
			<li>ZooKeeperManager<br/>
Node가 새로 Overlord에 등록할 때 상태 체크를 할 수 있도록 Node도 Zookeeper에 상태를 설정할 수 있게 하는 모듈이다.</li>
			<li>NodeTransferTask<br/>
Node끼리 데이터를 주고 받을 때 데이터를 받는 쪽에서 실행되는 모듈이다.</li>
			<li>NodeStateTask<br/>
주기적으로 Zookeeper의 해당 디렉토리에 자신의 상태를 업데이트 한다.</li>
			<li>NodeReceiveMasterTask<br/>
Node의 role이 slave일 때 주기적으로 Node에서 보내는 data dump를 받을 수 있도록 실행되는 모듈이다.</li>
			<li>NodeIntervalDumpTask<br/>
Node의 role이 master일 때 주기적으로 slave로 storage를 dump하여 보내도록 하는 모듈이다.</li>
			<li>Node<br/>
노드를 직접적으로 띄우고, 필요한 task들의 수행을 시작하는 모듈이다.
			<ul>
				<li>__process<br/>
Overlord에서 받은 request의 cmd에 따라 적절한 일을 수행하고 결과 값을 다시 돌려준다.</li>
				<li>__move_key<br/>
리밸런싱을 수행할 때 데이터를 줄 노드에서 수행되는 함수로, 각 요청된 키값에 대한 value를 storage에서 찾아서 옮길 key-value dictionary를 구성한다.</li>
				<li>__connect<br/>
Node가 처음 실행되었을 때 Overlord에 등록하기 위하여 수행되는 함수이다.</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
</li>
</ol>
<ol>
	<li>Implementation Issues
	<ul>
		<li>Node가 ms 간격으로 연속적으로 추가 될 때 매우 낮은 확률로 Node 고유값 또는 id가 겹칠 수 있는 경우가 있긴 하지만 처리 비용이 크기때문에 처리하지 않았다.</li>
		<li>Master Node와 Slave Node간에 동기화가 제대로 이루어 지지 않는 경우가 있었는데, 원인은 logging과 dumping의 시간차이, rebalancing후에 dumping이 다시 수행되지 않았던 점 등이 있고, 주기적으로 Master에서 Slave로 dump data를 보내는 것으로 해결했다.</li>
		<li>특정 Master 노드가 replication인 slave 노드를 하나도 가지고 있지 않은 상태에서 죽을때는 해당 데이터를 복구하지 못한다.</li>
		<li>코드 스타일에 있어서는 Thread를 과하다 싶을 정도로 썼고, ZeroMQ의 통신이 생각보다 정적이었기 때문에 통신부분 구현에 어려움을 겪었다.</li>
	</ul>
</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#9-result" id="user-content-9-result"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>9. Result</h2>
<ol>
	<li>Experiments
	<ol>
		<li>리밸런싱 테스트
		<ul>
			<li>실험 과정
			<ol>
				<li>Overlord 하나와(여러 개를 연결하는 것도 충분히 가능하다), Node 9개를 띄웠다.(설정은 1 Master 2 Slave 정책이다)</li>
				<li>해당 환경에서 5천개의 key-value를 삽입하는 스크립트를 실행하였다.</li>
				<li>데이터가 어떻게 쌓여 가는지, 균형은 잘 맞는지 등을 검사하였다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1120/Experiment-1.png" src="https://camo.githubusercontent.com/b470de3eff3b53f5f3179805510627bf52c045a1/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313132302f4578706572696d656e742d312e706e67"/>
</li>
			</ol>
</li>
			<li>실험 결과
			<ul>
				<li>Node에 데이터들이 잘 분산되고, 가장 키를 많이 가진 Node의 키 갯수가 가장 적게 가진 Node의 2배를 넘지 않는다는 것을 알 수 있었다.</li>
				<li>특정 Node에 데이터가 몰리는 경우, 리밸런싱 작업을 수행하여 비슷하게 Node 간 키의 갯수를 맞추는 것을 볼 수 있었다.</li>
				<li>Master Node와 Slave Node가 서로 동기화가 되어 같은 수를 유지하는 것을 확인할 수 있었다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1111/Experiment-1-1.png" src="https://camo.githubusercontent.com/34ec55b3d5dd6ddc9ed34bba153d5790877dc68a/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313131312f4578706572696d656e742d312d312e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1112/Experiment-1-2.png" src="https://camo.githubusercontent.com/7620a4fd3fce24a0d197579363af7f2004418a74/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313131322f4578706572696d656e742d312d322e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1113/Experiment-1-3.png" src="https://camo.githubusercontent.com/0249848cd46614fa399c085cef00cc28de59e6c1/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313131332f4578706572696d656e742d312d332e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1114/Experiment-1-4.png" src="https://camo.githubusercontent.com/2fdfc588e47e5a23522ab9f3b739aceda5eb76b5/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313131342f4578706572696d656e742d312d342e706e67"/>
</li>
			</ul>
</li>
		</ul>
</li>
		<li>Node 추가 실험
		<ul>
			<li>실험 과정
			<ol>
				<li>1개의 Overlord와 9개의 Node가 있는 상태에서, Node 하나를 추가하였다.</li>
				<li>위의 환경에서, Node를 하나 더 추가하였다.</li>
				<li>데이터 이동이 어떻게 이루어지는지, Node간 데이터 균형이 잘 맞는지 살펴보았다.</li>
			</ol>
</li>
			<li>실험 결과
			<ol>
				<li>처음에 추가한 Node는 Master Node가 된다. 이 때, 데이터가 잘 옮겨짐을 확인할 수 있었다.</li>
				<li>다음에 추가한 Node는 직전에 추가한 Master Node의 Slave Node가 된다. 곧 Master로부터 storage 내용을 전송받아 Master와 동기화가 이루어짐을 확인할 수 있었다. <br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1086/Experiment-2.png" src="https://camo.githubusercontent.com/1a97af9808dbcc7e9b7cef88cf60659b6ba34876/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313038362f4578706572696d656e742d322e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1087/Experiment-3.png" src="https://camo.githubusercontent.com/dd7ca9838f9165d05e2ee0d7accaa0c1febf3e1f/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313038372f4578706572696d656e742d332e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1119/Experiment-4.png" src="https://camo.githubusercontent.com/8bfa54b3220b934e22b8658628c5ec661a2b96dc/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313131392f4578706572696d656e742d342e706e67"/>
</li>
			</ol>
</li>
		</ul>
</li>
		<li>Node 제거 실험
		<ul>
			<li>실험 과정
			<ol>
				<li>Slave가 하나 있는 Master Node를 명령어를 이용하여 제거하였다.</li>
				<li>새로운 Master 임명이 어떻게 이루어지는지 확인하였다.</li>
			</ol>
</li>
			<li>실험 결과
			<ol>
				<li>Master Node를 제거하였을 때, 해당 Master Node의 Slave Node가 새로운 Master Node가 됨을 확인할 수 있었다.<br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1134/Experiment-5.png" src="https://camo.githubusercontent.com/dc35dc612d9cf164b0135207820b9dba3b035c34/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313133342f4578706572696d656e742d352e706e67"/><br/>
<img alt="" data-canonical-src="http://karma.snu.ac.kr:81/redmine/attachments/download/1135/Experiment-6.png" src="https://camo.githubusercontent.com/f44d78e572d48b1b3a216e5a37ebe970424859e4/687474703a2f2f6b61726d612e736e752e61632e6b723a38312f7265646d696e652f6174746163686d656e74732f646f776e6c6f61642f313133352f4578706572696d656e742d362e706e67"/>
</li>
			</ol>
</li>
		</ul>
</li>
	</ol>
</li>
	<li>Result Analysis and Discussion<br/>
10개 이상의 Node에 대해, 서비스가 잘 동작함을 확인할 수 있었다. 끊김 없이 잘 동작할 뿐 아니라, 리밸런싱을 통해 각 Node들의 데이터가 균등하게 유지되었다.<br/>
이번 프로젝트에서 가장 중요하게 여겼던 것은 서비스가 끊어지지 않고 노드의 추가가 가능해야 하고, 10% 이하의 노드가 더 이상 작동할 수 없을 경우에도 서비스에는 지장이 없어야 한다는 것이었다. DiMint는 해당 조건을 잘 만족하는 것으로 생각된다. 노드를 서비스 중에도 간단하게 새로 추가할 수 있고, 추가된 노드는 자동으로 자신의 역할을 부여받고 해당 역할에 따라 행동하게 된다. 노드가 삭제되는 경우에도, Slave가 삭제되는 경우에 잘 반영되고 Master가 삭제되는 경우 해당 Master Node의 Slave Node 중 하나가 새로운 Master Node로 임명된다.<br/>
다만 아쉬운 것은 초기 설정의 복잡성이다. DiMint는 Zookeeper의 기능을 십분 활용하는 서비스로, DiMint를 이용하기 위해선 필수적으로 Zookeeper가 미리 설치되어 있어야 한다. 하지만 Zookeeper를 실행하는 방법은 구글 등에서 검색해서 알아내야 할 정도로 직관적이지 않고 불편함이 존재하므로, 배포시 이를 고려하여 사용자가 쉽게 쓸 수 있게 하는 툴을 제공해야 할 것이다.</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#10-division--assignment-of-work" id="user-content-10-division--assignment-of-work"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>10. Division &amp; Assignment of Work</h2>
<ul>
	<li>김재찬
	<ul>
		<li>통신 프로토콜 정의</li>
		<li>Client 구현 총괄</li>
		<li>ClientEx 구현 총괄</li>
		<li>Overlord &amp; Node
		<ul>
			<li>네트워크 통신 구현</li>
			<li>Adding Node 구현</li>
			<li>Master election 구현</li>
			<li>Slave replication 구현</li>
		</ul>
</li>
		<li>스펙 발표</li>
	</ul>
</li>
	<li>유재성
	<ul>
		<li>프로젝트 일정 관리 및 issue 관리</li>
		<li>구조 디자인</li>
		<li>Client
		<ul>
			<li>모니터링 구현</li>
		</ul>
</li>
		<li>Overlord
		<ul>
			<li>기본 통신 구현</li>
			<li>ZooKeeper 리팩토링</li>
		</ul>
</li>
		<li>Overlord &amp; Node
		<ul>
			<li>state 구현</li>
			<li>배포 구현</li>
		</ul>
</li>
		<li>최종 발표</li>
	</ul>
</li>
	<li>염지혜
	<ul>
		<li>Overlord
		<ul>
			<li>네트워크 통신 구현</li>
			<li>Consistent hashing 구현</li>
			<li>Query offloading 구현</li>
			<li>Deleting Node 구현</li>
		</ul>
</li>
		<li>Overlord &amp; Node
		<ul>
			<li>데이터 리벨런싱 구현</li>
		</ul>
</li>
		<li>Node
		<ul>
			<li>노드간 통신 구현</li>
		</ul>
</li>
		<li>중간 발표</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#11-conclusion" id="user-content-11-conclusion"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>11. Conclusion</h2>
<p>본 프로젝트에서는 분산된 환경에서 대량의 key-value 데이터를 동적으로 밸런싱하여 메모리에 저장하는 저장소를 구현하였다. ZeroMQ를 사용한 간단한 패킷전송부터 구현하여, 분산된 환경에서도 안정적인 데이터 저장을 보장하는 zookeeper를 도입하여 Overlord와 Node를 동기적으로 관리하였다. 또한 key-value 데이터의 효율적인 rebalancing을 위해 consistent hashing을 구현하여 적용하였다.<br/>
본 프로젝트는 온라인으로 데이터 리밸런싱이 가능하기 때문에 리밸런싱 중에도 일부 키를 제외한 모든 키에 접근 가능한 저장소라는 점에서 의의를 갖는다. 또한 in-memory 저장소이기 때문에 반응 속도에서 우위를 점할 수 있다는 점과, zookeeper 베이스로 작성하여 안정성도 보장된다는 점도 의의가 있다. 추가적으로 배포하기 쉽도록 신경을 썼고, 모니터링 툴도 어느정도 완성도가 갖춰졌다는 것 또한 주목할만한 점이다. 노드 추가 요청이 ms 단위로 오는 등의 특수한 상황에 대한 처리를 엄밀하게 하지 못했던 점이 아쉽긴 하지만 실제로 사용하는 데에는 큰 문제가 없을 것으로 보인다.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#appendix-user-manual" id="user-content-appendix-user-manual"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>[Appendix] User Manual</h2>
<ul>
	<li>DiMint_Overlord
	<ul>
		<li>Installation<br/>
DiMint 서버는 ZooKeeper, Overlord와 Node로 구성되어 있습니다.<br/>
ZooKeeper는 구글링을 통해 쉽게 설치하실 수 있습니다.<br/>
아래의 유저 메뉴얼은 로컬 머신에서 ZooKeeper를 single instance로 실행한다고 가정합니다.<br/>
ZooKeeper를 설치하시고, Overlord source file를 받아주시기 바랍니다.<br/>
<pre><br/>
$ git clone https://github.com/dimint/dimint_overlord.git<br/>
$ cd dimint_overlord<br/>
</pre>
<br/>
python3로 작성되었기 때문에 python3가 설치된 환경에서 실행해주시기 바랍니다.<br/>
권한 문제를 해결하기 위해 virtualenv 환경에서 실행하였습니다.<br/>
<pre><br/>
$ virtualenv -p /usr/bin/python3 myenv<br/>
$ source ./myenv/bin/activate<br/>
$ python setup.py install<br/>
</pre>
<br/>
설치가 완료되었습니다. 다음의 명령어로 virtualenv 환경을 빠져나오실 수 있습니다.<br/>
다시 virtualenv 환경으로 가고 싶으시면 virtualenv가 설정된 myenv/bin/activate를 실행하시면 됩니다.<br/>
<pre><br/>
$ deactivate<br/>
</pre>
</li>
		<li>Execution<br/>
virtualenv 환경으로 진입한다음에 dimint 또는 dimint_overlord 로 overlord server를 띄울 수 있습니다.<br/>
물론 실행이 다 끝나고 deactivate 명령어로 virtualenv를 끝낼 수 있습니다.<br/>
dimint 를 치면 도움말이 나옵니다.<br/>
<pre><br/>
$ source ./myenv/bin/activate<br/>
$ dimint</pre>
</li>
	</ul>
</li>
</ul>
dimint help
dimint overlord help
dimint overlord list
dimint overlord start
dimint overlord stop
<p>$ dimint overlord help</p>
dimint overlord help
dimint overlord list
dimint overlord start
dimint overlord stop
<p>$ deactivate<br/>
<br/>
(이후 튜토리얼에서는 virtualenv 실행/해제는 보이지 않습니다.)<br/>
dimint overlord list는 현재 실행중인 python, dimint 프로그램을 모두 보여줍니다.<br/>
(따라서 로컬에서 실행중인 Node도 함께 보일 수 있습니다.)<br/>
프로그램을 종료시킬 때 pid가 필요하기 때문에 이용하면 편할 수 있습니다.<br/>
</p><pre><br/>
$ dimint_overlord list<br/>
psutil.Process(pid=21777, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_node/myenv/bin/python’, ‘/home/jsryu21/dimint_node/myenv/bin/dimint’, ‘node’, ‘start’]<br/>
psutil.Process(pid=22652, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_overlord/myenv/bin/python’, ‘/home/jsryu21/dimint_overlord/myenv/bin/dimint’, ‘overlord’, ‘list’]<br/>
</pre><br/>
dimint overlord start는 기본 config 파일을 읽어서 overlord를 띄웁니다.<br/>
기본 config 파일은 dimint_overlord/dimint_overlord.config 입니다.<br/>
다른 config 파일을 사용하고 싶으시면 dimint overlord start —config_path=another_config_path 이렇게 해주시면 됩니다.<br/>
따로 config 파일을 만들지 않고 설정값을 넣어서 overlord를 띄우고 싶으시면 dimint overlord start —port_for_client=5556 —port_for_node=5557 —zookeeper_hosts=115.71.237.6:2181 —hash_range=10000 —max_slave_count=2 이렇게 해주시면 됩니다.<br/>
overlord를 중단하고 싶으시면 dimint overlord stop pid 와 같이 적어주시되, pid 부분에 죽이고 싶은 overlord의 pid를 적으면 됩니다.<br/>
<pre><br/>
$ dimint overlord start<br/>
Hello from parent 23129 23131<br/>
$ dimint overlord list<br/>
psutil.Process(pid=21777, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_node/myenv/bin/python’, ‘/home/jsryu21/dimint_node/myenv/bin/dimint’, ‘node’, ‘start’]<br/>
psutil.Process(pid=23131, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_overlord/myenv/bin/python’, ‘/home/jsryu21/dimint_overlord/myenv/bin/dimint’, ‘overlord’, ‘start’]<br/>
psutil.Process(pid=23154, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_overlord/myenv/bin/python’, ‘/home/jsryu21/dimint_overlord/myenv/bin/dimint’, ‘overlord’, ‘list’]<br/>
$ dimint overlord stop 23131<br/>
$ dimint overlord start —config_path=dimint_overlord/dimint_overlord.config<br/>
Hello from parent 23201 23203<br/>
$ dimint overlord stop 23203<br/>
$ dimint overlord start —port_for_client=5556 —port_for_node=5557 —zookeeper_hosts=115.71.237.6:2181 —hash_range=10000 —max_slave_count=2<br/>
Hello from parent 23254 23256<br/>
$ dimint overlord stop 23256<br/>
$ dimint overlord list<br/>
psutil.Process(pid=21777, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_node/myenv/bin/python’, ‘/home/jsryu21/dimint_node/myenv/bin/dimint’, ‘node’, ‘start’]<br/>
psutil.Process(pid=23292, name=‘dimint’), cmdline : [‘/home/jsryu21/dimint_overlord/myenv/bin/python’, ‘/home/jsryu21/dimint_overlord/myenv/bin/dimint’, ‘overlord’, ‘list’]<br/>
</pre><br/>
다른 프로그램인 dimint_overlord는 dimint overlord 명령어의 줄임버전입니다.<br/>
—config_path로 옵션으로 다른 config를 넣어줄 수 있고, config_path 없이도 —port_for_client=5556 —port_for_node=5557 —zookeeper_hosts=127.0.0.1:2121 —hash_range=10000 —max_slave_count=2 이렇게 옵션을 주어 실행할 수 있습니다.<br/>
단, dimint 프로그램처럼 fork로 띄우는 식이 아니라 바로 띄우기 때문에 shell을 차지합니다.<br/>
따라서 ctrl+c를 통해 프로그램을 종료시킬 수 있습니다.<br/>
<pre><br/>
$ dimint_overlord<br/>
ip : 115.71.237.6<br/>
OverlordStateTask works<br/>
^CException ignored in: &lt;module ‘threading’ from ‘/usr/local/lib/python3.4/threading.py’&gt;<br/>
Traceback (most recent call last):<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1294, in <em>shutdown<br/>
    t.join()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1060, in join<br/>
    self._wait_for_tstate</em>lock()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1076, in <em>wait_for_tstate_lock<br/>
    elif lock.acquire(block, timeout):<br/>
KeyboardInterrupt<br/>
$ dimint_overlord —config_path=dimint_overlord/dimint</em>overlord.config<br/>
ip : 115.71.237.6<br/>
OverlordStateTask works<br/>
^CException ignored in: &lt;module ‘threading’ from ‘/usr/local/lib/python3.4/threading.py’&gt;<br/>
Traceback (most recent call last):<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1294, in <em>shutdown<br/>
    t.join()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1060, in join<br/>
    self._wait_for_tstate</em>lock()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1076, in <em>wait_for_tstate_lock<br/>
    elif lock.acquire(block, timeout):<br/>
KeyboardInterrupt<br/>
$ dimint_overlord —port_for_client=5556 —port_for_node=5557 —zookeeper_hosts=127.0.0.1:2121 —hash_range=10000 —max_slave</em>count=2<br/>
ip : 115.71.237.6<br/>
OverlordStateTask works<br/>
^CException ignored in: &lt;module ‘threading’ from ‘/usr/local/lib/python3.4/threading.py’&gt;<br/>
Traceback (most recent call last):<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1294, in <em>shutdown<br/>
    t.join()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1060, in join<br/>
    self._wait_for_tstate</em>lock()<br/>
  File “/usr/local/lib/python3.4/threading.py”, line 1076, in _wait_for_tstate_lock<br/>
    elif lock.acquire(block, timeout):<br/>
KeyboardInterrupt<br/>
</pre>

	<ul>
		<li>Configuration
		<ul>
			<li>port_for_client: port for receive client’s request and send to response to client. default is 5556.</li>
			<li>port_for_node: port for send client’s request to node and receive node’s request. default is 5557,</li>
			<li>zookeeper_hosts: zookeeper host list. If zookeeper is multiple, each host is separated by comma. default is “127.0.0.1:2181”.</li>
			<li>hash_range: key hash range. all keys are hashed and stored to proper nodes. default is 10000.</li>
			<li>max_slave_count: maximum slave count for each master node. default is 2.</li>
		</ul>
</li>
		<li>Etc.
		<ul>
			<li>dimint 또는 dimint_overlord 프로그램이 오동작하는 경우가 있습니다. ps와 kill 명령어를 통해 강제종료시켜주시면 됩니다.</li>
			<li>ZooKeeper에 쓰레기 값이 남아있어 오동작하는 경우가 있습니다. python test.py 명령어로 현재 ZooKeeper 상태를 확인할 수 있습니다.</li>
			<li>또, python test.py clear 명령어로 현재 ZooKeeper 에 있는 쓰레기 값을 지울 수 있습니다.<br/>
<pre><br/>
$ python test.py<br/>
overlord list : [‘115.71.237.6:5556’]<br/>
node list : [‘ee4db710c82b312e0eee62ac9da024db’]<br/>
master node : ee4db710c82b312e0eee62ac9da024db<br/>
$ python test.py clear<br/>
$ python test.py<br/>
overlord list : []<br/>
node list : []<br/>
</pre>
</li>
		</ul>
</li>
	</ul>
	<li>DiMint_Node<br/>
DiMint Ndoe는 DiMint Storage에서 실제로 key-value를 저장하는 기능을 담당합니다.
	<ul>
		<li>PreRequirements
		<ul>
			<li>Python3 이상</li>
		</ul>
</li>
		<li>Installation<br/>
해당 저장소를 다운받고, 해당 폴더로 이동합니다.<br/>
<pre><br/>
$ git clone https://github.com/DiMint/DiMint_Node<br/>
$ cd DiMint_Node<br/>
</pre>
<br/>
setup.py를 선택하여 실행합니다. Python 2에서는 정상 작동이 보장되지 않으므로, Python 3를 이용하는 것을 권장합니다.<br/>
<pre><br/>
$ python setup.py install<br/>
</pre>
</li>
		<li>Execution<br/>
다음 명령어로 도움말을 볼 수 있습니다.<br/>
<pre><br/>
$ dimint node help<br/>
    dimint help<br/>
    dimint node help<br/>
    dimint node list<br/>
    dimint node start<br/>
    dimint node stop<br/>
</pre>
<br/>
다음 명령어로 Node를 실행할 수 있습니다. 이 때, node process는 fork되어 background에서 작동합니다.<br/>
<pre><br/>
$ dimint node start<br/>
Hello from parent 27346 27355<br/>
</pre>
<br/>
이 때, 두 번째 숫자가 실행된 node의 pid입니다.<br/>
또는 다음과 같이 실행할 수도 있습니다.<br/>
<pre><br/>
$ dimint_node start<br/>
</pre>
<br/>
이렇게 실행하면 해당 shell에서 node가 바로 실행됩니다.<br/>
이 때 기본 설정 파일은 dimint_node/dimint_node.config 파일입니다. 만일 다른 config 파일을 이용하고 싶다면, 다음과 같이 실행하시면 됩니다.<br/>
<pre><br/>
$ dimint node start —config_path=some_file_path<br/>
</pre>
<br/>
설정 파일은 다음과 같은 구조로 이루어져야 합니다.<br/>
<pre><br/>
{<br/>
    “host”: “127.0.0.1”,<br/>
    “port”: 5557,<br/>
    “pull_port”: 15558,<br/>
    “push_to_slave_port”: 5558,<br/>
    “receive_to_slave_port”: 5600,<br/>
    “transfer_port”: 5700<br/>
}<br/>
</pre>
<br/>
따로 설정 파일을 두지 않고 커맨드 라인에서 바로 이용하시려면 다음과 같이 인자를 넣으면 됩니다.<br/>
<pre><br/>
$ dimint_node —host=127.0.0.1 —port=5557 —pull_port=15558 —push_to_slave_port=5558 —receive_to_slave_port=5600 —transfer_port=5700<br/>
</pre>
<br/>
중단하기 위해서는 다음 명령을 입력합니다.<br/>
<pre><br/>
$ dimint node stop <br/>
</pre>
<br/>
또는 다음과 같이 할 수도 있습니다.<br/>
<pre><br/>
$ kill -USR1 <br/>
</pre>
</li>
		<li>Configuration
		<ul>
			<li>host: overlord host. default is ‘127.0.0.1’</li>
			<li>port: overlord port. default is 5557</li>
			<li>pull_port: port for receive overlord’s request. default is 15558.</li>
			<li>push_to_slave_port: port for send request to it’s slaves. default is 5558.</li>
			<li>receive_slave_port: port for receive slave’s request. default is 5600.</li>
			<li>transfer_port: port for transfer data to other node. default is 5700.</li>
		</ul>
</li>
	</ul>
</li>
	<li>DiMint_Client
	<ul>
		<li>Installation<br/>
<pre><br/>
$ virtualenv -p /usr/bin/python3 myenv<br/>
$ source myenv/bin/activate<br/>
$ pip install -e git+git://github.com/DiMint/DiMint_Client@master#egg=dimint_client.egg<br/>
$ deactivate<br/>
</pre>
</li>
		<li>How to use<br/>
<pre><br/>
$ source myenv/bin/activate<br/>
</pre>
<br/>
<pre><br/>
from dimint_client import DiMintClient</pre>
</li>
	</ul>
<p>dimint = DiMintdimint(‘127.0.0.1’, 5556)</p>
<ol>
	<li>Get overlord host list<br/>
print(dimint.get_overlord_list())</li>
</ol>
<ol>
	<li>Get Value from DiMint Server<br/>
value1 = dimint.get(‘key1’) # dimint[‘key1’] also work</li>
</ol>
<ol>
	<li>Set Value to DiMint Server<br/>
dimint.set(‘key2’, [1,2,3,4,5]) # dimint[‘key2’] = [1,2,3,4,5] also work</li>
</ol>
<ol>
	<li>Get states of all nodes<br/>
states = dimint.get_state()<br/>
</li>
	<li>DiMint_ClientEx<br/>
DiMint Overlord와 DiMint Node의 상태를 확인하고, 간단하게 값을 추가하거나 확인할 수 있는 웹 페이지입니다.
	<ul>
		<li>Installation<br/>
먼저 해당 저장소를 git clone을 통해 다운받고, 폴더로 이동합니다.<br/>
<pre><br/>
$ git clone https://github.com/DiMint/DiMint_ClientEx<br/>
$ cd DiMint_ClientEx<br/>
</pre>
<br/>
필요한 패키지를 설치합니다.<br/>
<pre><br/>
$ pip3 install -r requirements.txt<br/>
</pre>
<br/>
config.cfg.example 파일을 config.cfg로 이름을 바꾸거나, 복사합니다.<br/>
<pre><br/>
$ cp config.cfg.example config.cfg<br/>
</pre>
<br/>
config.cfg 파일을 변경합니다. <span>DIMINT</span>\<em>HOST와 <span>DIMINT</span>\</em>PORT가 있는데, 띄운 DiMint Overlord의 호스트 주소, 포트를 입력하면 됩니다.</li>
		<li>Execution<br/>
app.py 파일을 실행합니다.<br/>
<pre><br/>
$ python app.py<br/>
</pre>
<br/>
이제 웹 브라우저에서 http://localhost:5000 으로 들어가시면 웹 사이트를 볼 수 있습니다.</li>
		<li>Use
		<ul>
			<li>Monitor 탭에서는 현재 Overlord에 접속해 있는 노드들의 상태를 볼 수 있습니다. 각 노드에 할당된 키의 갯수를 하나의 차트로 보여주고, 각 노드의 시간별 키 갯수/메모리 사용량을 그 아래에 그래프로 보여줍니다.</li>
			<li>Get/Set 탭에서는 간단하게 특정 key에 있는 값을 가져올 수 있고, 특정 key에 값을 넣을 수도 있습니다. 현재 Increment, Decrement는 작동하지 않습니다.</li>
		</ul>
</li>
	</ul>
</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#appendix-references" id="user-content-appendix-references"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>[Appendix] References</h2>
<ul>
	<li>NoSQL http://en.wikipedia.org/wiki/NoSQL
	<ul>
		<li>Distributed Algorithms in NoSQL Databases https://highlyscalable.wordpress.com/2012/09/18/distributed-algorithms-in-nosql-databases/</li>
	</ul>
</li>
	<li>Redis http://redis.io/
	<ul>
		<li>Redis cluster Specification (work in progress) http://redis.io/topics/cluster-spec</li>
		<li>Partitioning: how to split data among multiple Redis instances. http://redis.io/topics/partitioning</li>
		<li>In memory dictionary Redis 소개 – http://bcho.tistory.com/654</li>
	</ul>
</li>
	<li>Free &amp; open source, high-performance, distributed memory object caching system http://www.memcached.org/</li>
	<li>MongoDB http://www.mongodb.com/</li>
	<li>ZeroMQ Concurrency Framework로써 수행가능한 소켓 라이브러리 http://kr.zeromq.org/
	<ul>
		<li>zmq – http://zeromq.github.io/pyzmq/api/zmq.html</li>
		<li>Advanced Request-Reply Patterns http://zguide.zeromq.org/php:chapter3#Chapter-Advanced-Request-Reply-Patterns</li>
	</ul>
</li>
	<li>ZooKeeper http://zookeeper.apache.org/
	<ul>
		<li>손쉽게 사용하는 ZooKeeper 스토리지, Zoopiter! http://helloworld.naver.com/helloworld/textyle/583580</li>
		<li>ZooKeeper를 활용한 Redis Cluster 관리 http://helloworld.naver.com/helloworld/294797</li>
	</ul>
</li>
	<li>Consistent hashing http://en.wikipedia.org/wiki/Consistent_hashing</li>
	<li>Multi-master replication http://en.wikipedia.org/wiki/Convolutional_neural_network</li>
	<li>Query Offloading for Improved Performance and Better Resource Utilization http://www.oracleimg.com/ocom/idcplg?IdcService=GET_FILE&amp;dID=332542&amp;dDocName=336615</li>
	<li>[ZooKeeper] (0) zookeepr는 무엇인가? http://blog.seulgi.kim/search/label/zookeeper</li>
</ul>
</li>
        </div>

    </div>]