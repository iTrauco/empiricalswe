[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#rosimagesopencv" id="user-content-rosimagesopencv"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>ROS/Images/OpenCV</h1>
<h3>
<a aria-hidden="true" class="anchor" href="#useful-links" id="user-content-useful-links"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Useful links</h3>
<ul>
<li>
<p><a href="http://wiki.ros.org/image_pipeline" rel="nofollow"><em><strong><code>image_pipeline</code></strong></em></a> is the stack for dealing with ROS Image messages. <em>The packages below</em> are part of <code>image_pipeline</code>:</p>
<ol>
<li>
<a href="http://wiki.ros.org/image_view" rel="nofollow"><code>image_view</code></a>
<ul>
<li>Package used to view ROS Image messages.</li>
</ul>
</li>
<li>
<a href="http://wiki.ros.org/image_proc?distro=indigo" rel="nofollow"><code>image_proc</code></a>
<ul>
<li>Package used to process images (often takes input from camera driver or rosbag) and undistorts it. See <a href="http://wiki.ros.org/stereo_image_proc" rel="nofollow">stereo_image_proc</a> for the stereo equivalent.</li>
</ul>
</li>
<li>
<a href="http://wiki.ros.org/camera_calibration?distro=indigo" rel="nofollow"><code>camera_calibration</code></a>
<ul>
<li>Package used to calibrate the camera (to get intrinsic parameters).</li>
<li>After calibrating the camera, the camera node (i.e. <code>usb_cam</code>) will take care of publishing the calibration information for that camera on the <code>/camera_info</code> topic along with the raw image topic.</li>
</ul>
</li>
</ol>
<ul>
<li>
<a href="http://en.wikipedia.org/wiki/Camera_resectioning#Intrinsic_parameters" rel="nofollow">Intrinsic camera parameters</a> are those that are independent of the camera position or orientation. They are essentially focal length and the distortion coefficients.</li>
<li>
<a href="http://en.wikipedia.org/wiki/Camera_resectioning#Extrinsic_parameters" rel="nofollow">Extrinsic camera parameters</a> are those that describe the "coordinate system transformations from 3D world coordinates to 3D camera coordinates." In other words, how to turn the pixels in an image to points in 3D space.</li>
</ul>
</li>
<li>
<p><a href="http://wiki.ros.org/image_transport" rel="nofollow"><code>image_transport</code></a> is the hidden layer underneath nodes that use ROS messages. Essentially, it takes care of publishing all the extra "topics" for the different compression formats (<code>image_transport</code> doesn't use ROS topics, but something very similar and transparent for the user).</p>
<ul>
<li>ROS <code>Image</code> messages can be in RAW format (not encoded or compressed), or they can be compressed (in JPEG format mostly), in which case they would be <code>CompressedImage</code> ROS message types. In order to take care of all this nonsense compression magic, <code>image_transport</code> is used as a hidden layer underneath nodes that use ROS <code>Image</code> messages (such as <code>image_view</code> or <code>image_proc</code>).</li>
</ul>
</li>
<li>
<p><a href="http://www.cse.sc.edu/%7Ejokane/teaching/574/notes-images.pdf" rel="nofollow">Notes on Images in ROS</a></p>
</li>
<li>
<p>Besides the image_resizer we have, you can also compress/decompress images in rosbags by playing them and recording them at the same time using the different transport (compressed or not). Also, check out <a href="https://github.com/ros/rosbag_image_compressor">this</a> image_compressor node (this one uses PNG compression).</p>
</li>
</ul>
<h1>
<a aria-hidden="true" class="anchor" href="#cameras-and-lenses" id="user-content-cameras-and-lenses"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Cameras and Lenses</h1>
<ul>
<li>
<p>List of companies:</p>
<ul>
<li><a href="http://www.ptgrey.com/camera-selector" rel="nofollow">Pointgrey</a></li>
<li><a href="http://www.axis.com/products/cam_acc/addon/lenses/cs_lenses/index.htm" rel="nofollow">Axis</a></li>
<li><a href="http://www.edmundoptics.com/" rel="nofollow">Edmund Optics</a></li>
<li><a href="http://www.thorlabs.de/navigation.cfm?guide_id=2025" rel="nofollow">Thor Labs</a></li>
</ul>
</li>
<li>
<p>Read all this about <a href="http://www.ptgrey.com/KB/10694" rel="nofollow">lenses</a> and <a href="http://www.wikihow.com/Choose-a-Lens-Aperture-%28F-Stop%29" rel="nofollow">here also</a>.</p>
</li>
<li>
<p>What to lookout for:</p>
<ul>
<li>The lens <code>focal length</code> and the camera <code>image sensor</code> size determine the <code>field of view</code> you get.</li>
<li>Also watch out for <code>aperture size</code> and <code>vignetting</code>.</li>
</ul>
</li>
<li>
<p>How to calculate <a href="https://en.wikipedia.org/wiki/Angle_of_view" rel="nofollow">Angle of view</a>:</p>
<ul>
<li>
<p>First find the image sensor dimensions (check link below).</p>
</li>
<li>
<p>Then find the lens focal length.</p>
</li>
<li>
<p>Then plug them into this equation:</p>
<p><img alt="angle of view" data-canonical-src="https://upload.wikimedia.org/math/c/9/6/c9652413291248e3eb7413571cadb79c.png" src="https://camo.githubusercontent.com/2b5b1c32e95545efcdff77fe14f76dc924cdbbb8/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f6d6174682f632f392f362f63393635323431333239313234386533656237343133353731636164623739632e706e67"/></p>
</li>
</ul>
</li>
<li>
<p>Don't worry too much about lens mount types because there's adapters out there. Usually C-mount or CS-mount are what we'll need. (CS cam and C lens can be used with a spacer, but not vice versa)</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Image_sensor_format#Table_of_sensor_formats_and_sizes" rel="nofollow">List of image sensor dimensions</a>.</p>
</li>
<li>
<p>Possibly use DSLR cameras: use <a href="http://magiclantern.wikia.com/wiki/Remote_control_with_PTP_and_Python" rel="nofollow">Python scripts</a> to remote control cameras.</p>
</li>
<li>
<p><a href="http://www.ptgrey.com/KB/10028" rel="nofollow">Global vs Rolling shutter</a></p>
</li>
<li>
<p><a href="http://electronics.howstuffworks.com/cameras-photography/digital/question362.htm" rel="nofollow">CCD vs CMOS</a> sensor types</p>
</li>
<li>
<p><a href="http://www.axis.com/products/video/camera/about_cameras/iris.htm" rel="nofollow">Iris features</a> (basically controllable aperture. Auto-iris works best for changing lighting conditions but might suffer diffraction lens flare artifacts under bright light)</p>
</li>
</ul>
<h1>
<a aria-hidden="true" class="anchor" href="#general-overview" id="user-content-general-overview"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>General Overview</h1>
<p><strong>ignore this section for now</strong></p>
<h3>
<a aria-hidden="true" class="anchor" href="#first-approach" id="user-content-first-approach"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>First Approach</h3>
<ul>
<li>Stereo camera driver node publishes raw image topics.</li>
<li>stereo_image_proc subscribes to these images and publishes a pointcloud of everything.</li>
<li>Some node listens to pointcloud and separates ground points and above-ground points.</li>
<li>The ground points are thrown into line_detection, which has to figure out which pixels these points came from (reconstruct an image of the ground), and then find the lines in that ground image. Once the lines are found in the image, we pick the points that correspond to these line pixels and publish those points into the costmap</li>
<li>The above-ground points are thrown into costmap as barrels and obstacles.
<img alt="diagram of first method" data-canonical-src="https://copy.com/WfK0Uam84Ja4" src="https://camo.githubusercontent.com/f4294357ba1d86548283f6e9f96700821ae24fee/68747470733a2f2f636f70792e636f6d2f57664b3055616d38344a6134"/>
</li>
</ul>
<h3>
<a aria-hidden="true" class="anchor" href="#second-approach" id="user-content-second-approach"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Second Approach</h3>
<ul>
<li>Stereo camera driver node publishes raw image topics.</li>
<li>line_detection subscribes to raw image and publishes image with only lines. stereo_image_proc1 then subscribes to the line images and publishes line pointcloud to costmap (line layer).</li>
<li>in parallel, stereo_image_proc2 <strong>also</strong> subscribes to raw images and publishes pointcloud of everything. A ground_chopper node subscribes to this pointcloud and chops off ground (publishes above ground pointcloud to costmap obstacle layer)
<img alt="diagram of second method" data-canonical-src="https://copy.com/NUfDJBVFNGyG" src="https://camo.githubusercontent.com/dbd02e8a4c40fc50a5174c4ac44be15b1986b150/68747470733a2f2f636f70792e636f6d2f4e5566444a4256464e477947"/>
</li>
</ul>

        </div>

    </div>]