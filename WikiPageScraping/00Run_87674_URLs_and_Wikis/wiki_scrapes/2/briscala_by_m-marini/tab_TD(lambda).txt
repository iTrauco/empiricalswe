[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#tdlambda" id="user-content-tdlambda"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>TD(lambda)</h1>
<p>Definiamo <code>R(n)(t)</code> il ritorno aspettato a n-step come segue:</p>
<pre><code>R(n)(t) = Sum_i gamma^i r(t+i) + gamma^n V(s(t+1) ), i = 0 ... n - 1
</code></pre>
<p>Con questa definizione possiamo calcolare <code>R(Lambda)</code> il ritorno aspettato lambda come segue:</p>
<pre><code>R(lambda)(t) = (1 - lambda) Sum_i lambda^(n-1) R(i)(t), i = 0 ... T
</code></pre>
<p><code>T</code> è l'ultimo step dell'episodio.</p>
<p>In generale l'algoritmo TD(lambda) prevede l'aggiornamento delle funzioni di valore dello stato V(s) e Q(s,a) come segue:</p>
<pre><code>d V(s(t)) = alpha (R(lambda)(t) - V(s(t)) )
d Q(s(t), a(t)) = alpha (R(lambda)(t) - Q(s(t), a(t)) )
</code></pre>
<h2>
<a aria-hidden="true" class="anchor" href="#discesa-del-gradiente" id="user-content-discesa-del-gradiente"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Discesa del gradiente</h2>
<p>Nel caso le funzioni di stato e azione siano funzioni approssimate con parametri <code>Theta</code>:</p>
<pre><code>V(s) = V(VTheta)(s)
Q(s, a) = V(QTheta)(s, a)
</code></pre>
<p>L'ottimizzazione avviene con il metodo della <a href="http://it.wikipedia.org/wiki/Discesa_del_gradiente" rel="nofollow">discesa del gradiente</a>.<br/>
In tal caso l'operazione di apprendimento consiste nel modificare i parametri delle funzioni
in base all'errore:</p>
<pre><code>d VTheta(s(t)) = alpha (R(lambda)(t) - V(s(t)) ) grad V(s(t)) / d VTheta
d QTheta(s(t), a(t)) = alpha (R(lambda)(t) - Q(s(t), a(t)) ) grad Q(s(t),a(t)) / d QTheta
</code></pre>
<p>L'algoritmo che usa la backward view è</p>
<pre><code>d VTheta(s(t)) = alpha  VErr(s(s)) EV(t)
d QTheta(s(t), a(t)) = alpha  QErr(s(t), a(t)) EQ(t)

VErr(s(t)) = R(t) - V(s(t))
QErr(s(t), a(t)) = R(t) - Q(s(t), a(t))

d EV(t) = - EV(t) (1 - gamma lambda) + grad V(s(t)) / d VTheta
d EQ(t) = - EQ(t) (1 - gamma lambda) + grad Q(s(t)) / d QTheta

EV(1) = EQ(1) = 0
</code></pre>
<p><code>EV</code> ed <code>EV</code> sono i vettori di eleggibilità della funzione di stato e di azione.</p>
<p>Per il caso della briscola abbiamo:</p>
<pre><code>d EV(t) = - EV(t) (1 - lambda) + grad V(s(t)) / d VTheta
d EQ(t) = - EQ(t) (1 - lambda) + grad Q(s(t)) / d QTheta
</code></pre>

        </div>

    </div>]