[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#apprendimento" id="user-content-apprendimento"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Apprendimento</h1>
<p>Lo scopo della fase di apprendimento è quella di creare una nuova rete neurale
che implementa una strategia di gioco migliore imparando giocando nuove partite.</p>
<p>Il programma di apprendimento prende una rete neurale in ingresso, i parametri di
apprendimento e di regolarizzazione e produce ad ogni ciclo una nuova rete e gli indicatori
di monitoraggio.</p>
<p>Il ciclo di apprendimento consiste nella generazione di una partita applicando la strategia
implementata dalla rete iniziale (strategia applicata da tutti e due i giocatori).</p>
<p>Alla fine della partita si genera una nuova rete applicando l'algoritmo di apprendimento
aggiornato con i dati della nuova partita e si calcolano gli indicatori di monitoraggio.</p>
<p>La nuova rete prodotta viene salvata su file mentre gli indicatori vengono usati per
generare i grafici di monitoraggio (learning curve, convergence curve, ...).</p>
<h2>
<a aria-hidden="true" class="anchor" href="#competizione-di-strategie" id="user-content-competizione-di-strategie"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Competizione di strategie</h2>
<p>Possiamo usare due policy diverse per le strategie di gioco dei due giocatori.
In questo modo possiamo mettere a confronto due strategie diverse e tenere sempre la migliore.</p>
<p>Avremo quindi una strategia di riferimento <code>pi_ref</code> e una in addestramento <code>pi_t</code>. Quando la strategia in
addestramento risulta migliore di quella di riferimento sostituisce quella di riferimento.</p>
<p>L'iterazione generalizzata della strategia (GPI Generalized Policy Iteration) è quindi formata da
cicli con una fase di apprendimento dove una rete viene addestrata su un campione di partite
(training set), una fase di valutazione delle due strategie su un altro campione di partite
(validation set) ed infine una fase di selezione della rete migliore.</p>
<p>La strategia migliore viene selezionata prendendo quella con il numero maggiore di partite vinte
durante la fase di valutazione.</p>
<p>La strategia in addestramento è basata su un'algoritmo <code>epsilon</code>-<em>greedy</em>: con probabilità <code>epsilon</code>
viene scelta un'azione casuale) e con probabilità <code>1 - epsilon</code> si sceglie l'azione migliore.
Questo permette sia di esplorare nuove strategie che sfruttare comportamenti virtuosi appresi (exploration vs exploitation).</p>
<p>D'altra parte la strategia di riferimento invece non necessita di esplorare nuove possibilità,
quindi viene usato un algoritmo <em>greedy</em>, selezionando sempre l'azione migliore</p>

        </div>

    </div>]