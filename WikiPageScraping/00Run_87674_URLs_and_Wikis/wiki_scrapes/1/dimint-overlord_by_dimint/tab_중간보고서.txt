[<div class="wiki-body gollum-textile-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#%EC%A4%91%EA%B0%84%EB%B3%B4%EA%B3%A0%EC%84%9C" id="user-content-중간보고서"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>중간보고서</h1>
<p>Nexon 분산된 key-value storage<br/>
프로젝트 Team J</p>
<p>컴퓨터공학부 2008-11572 김재찬<br/>
컴퓨터공학부 2008-11662 유재성<br/>
컴퓨터공학부 2012-11256 염지혜</p>
<p>엄현상 교수님</p>
<p>{{toc}}</p>
<h2>
<a aria-hidden="true" class="anchor" href="#1-abstract" id="user-content-1-abstract"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>1. Abstract</h2>
<p>본 프로젝트에서는 분산된 key-value storage를 구현하려고 한다. 분산된 환경 구성을 위해 Zookeeper과 Consistent Hashing을 이용하고, 다양한 문제 상황에 대비하기 위해 Master-Slave model을 도입했다. 편의성을 위해 다양한 타입을 지원하고, 서비스를 중단하지 않고 node를 추가/제거할 수 있게 하며, 이론상 수백개의 node를 한 서비스에서 이용할 수 있게 하는 것이 본 프로젝트의 목표이다.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#2-introduction" id="user-content-2-introduction"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>2. Introduction</h2>
<ol>
	<li>Introduction
	<ul>
		<li>분산된 노드로 구성된 대량의 key-value형 데이터를 밸런싱하여 저장하는 안정된 저장소를 구현한다.</li>
		<li>저장소와 저장소에 접근하기 위한 라이브러리 그리고 해당 라이브러리를 이용한 간단한 클라이언트를 구성한다.</li>
	</ul>
</li>
	<li>References
	<ul>
		<li>NoSQL : http://en.wikipedia.org/wiki/NoSQL</li>
		<li>Redis : http://redis.io/</li>
		<li>Redis cluster Specification (work in progress) : http://redis.io/topics/cluster-spec</li>
		<li>Free &amp; open source, high-performance, distributed memory object caching system : http://www.memcached.org/</li>
		<li>MongoDB : http://www.mongodb.com/</li>
		<li>ZeroMQ Concurrency Framework로써 수행가능한 소켓 라이브러리 : http://kr.zeromq.org/</li>
		<li>ZooKeeper : http://zookeeper.apache.org/</li>
		<li>Consistent hashing : http://en.wikipedia.org/wiki/Consistent_hashing</li>
		<li>Multi-master replication : http://en.wikipedia.org/wiki/Convolutional_neural_network</li>
		<li>Query Offloading for Improved Performance and Better Resource Utilization : http://www.oracleimg.com/ocom/idcplg?IdcService=GET_FILE&amp;dID=332542&amp;dDocName=336615</li>
	</ul>
</li>
	<li>Definitions
	<ul>
		<li>key-value store : Key-value (KV) stores use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.(http://en.wikipedia.org/wiki/NoSQL#Key-value_stores)</li>
		<li>Relational database : A relational database is a database that stores information about both the data and how it is related.(http://en.wikipedia.org/wiki/Relational_database)</li>
		<li>NoSQL : A NoSQL or Not Only <span>SQL</span> database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases.(http://en.wikipedia.org/wiki/NoSQL)</li>
		<li>Distributed data store : A distributed data store is a computer network where information is stored on more than one node, often in a replicated fashion.(http://en.wikipedia.org/wiki/Distributed_data_store)</li>
		<li>Consistent hashing : Consistent hashing is a special kind of hashing such that when a hash table is resized and consistent hashing is used, only K/n keys need to be remapped on average, where K is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped.(http://en.wikipedia.org/wiki/Consistent_hashing)</li>
		<li>Partition tolerance : the system continues to operate despite arbitrary message loss or failure of part of the system(http://en.wikipedia.org/wiki/CAP_theorem)</li>
	</ul>
</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#3-background-study" id="user-content-3-background-study"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>3. Background Study</h2>
<ol>
	<li>관련 접근방법/기술 장단점 분석<br/>
본 프로젝트는 크게 In Memory Storage 와 Distributed System 이라는 두가지 키워드에 집중한다.
	<ul>
		<li>In Memory Storage<br/>
메모리를 저장소로 쓰는 데이터베이스는 디스크를 저장소로 쓰는 데이터베이스에 비해 성능이 크게 뛰어나다. 하지만 아직까지 분산처리 기능을 완벽히 지원하는 In memory storage product가 없어 분산처리 환경에서 사용하기엔 아직 불편하다.</li>
		<li>Distributed System<br/>
저장하고 처리 해야 할 데이터의 양이 늘어남에 따라 효율적인 분산 처리 방법에 대한 연구가 활발히 진행되고 있는데, 데이터베이스의 경우 부하를 줄이기 위해 scale-out storage를 많이 사용한다. scale-out storage는 여러개의 노드를 저장소로 쓰고, 데이터 처리량을 적절히 나누어 각각의 저장소에 가해지는 부하가 균형있게 배분되도록 하는 저장소이다. 이 때 데이터 처리를 나누기 위해 많이 쓰는 방법이 sharding 이다. 노드 갯수를 정해놓고, static sharding을 하는 것이 구현하기 간단하기 때문에 널리 쓰이고 있다. 그러나 static sharding은 데이터 처리량에 따라 노드를 추가하거나 제거할 때 데이터를 재배치해야 하기 때문에 비용이 크고, 동적으로 수행할 수 없다는 단점이 있다.</li>
	</ul>
</li>
	<li>프로젝트 개발환경</li>
	<li>Os
	<ul>
		<li>unix (ubuntu 14.04)</li>
	</ul>
</li>
	<li>Programming Language
	<ul>
		<li>Python 3
		<ul>
			<li>For flexible type system</li>
		</ul>
</li>
	</ul>
</li>
	<li>Library
	<ul>
		<li>ZeroMQ
		<ul>
			<li>For end-to-end communication</li>
		</ul>
</li>
		<li>Zookeeper
		<ul>
			<li>Managing Nodes</li>
		</ul>
</li>
		<li>Flask
		<ul>
			<li>For Dimint Test Client</li>
		</ul>
</li>
	</ul>
</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#4-goalproblem--requirements" id="user-content-4-goalproblem--requirements"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>4. Goal/Problem &amp; Requirements</h2>
<p>본 프로젝트에서는</p>
<ul>
	<li>분산된 환경에서</li>
	<li>대량의 key-value 데이터를</li>
	<li>동적으로 밸런싱하여</li>
	<li>메모리에 저장하는 저장소를</li>
</ul>
<p>구현하고자 한다.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#5-approach" id="user-content-5-approach"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>5. Approach</h2>
<p>데이터를 동적으로 밸런싱하기 위해 Consistent Hashing 을 사용하기로 했다. Consistent Hashing은 하나의 노드가 일정 범위의 해쉬값을 가지는 데이터들을 각각 처리하도록 하는 모델이다. 이 모델에서 노드가 추가되면 인접한 다른 노드에서 적절한 양의 데이터를 할당받고, 노드가 제거되면 마찬가지로 인접한 다른 노드들이 제거된 노드의 데이터를 적절히 나눠 할당받기 때문에 다른 노드들은 영향을 받지 않는다는 장점이 있다.</p>
<p>{{thumbnail(그림1.png, size=600, title=Thumbnail)}}</p>
<p>본 프로젝트의 저장소 시스템은 일부 노드가 정상 작동하지 않아도 전체 시스템이 정상적으로 작동되어야 하기 때문에 각 노드마다 해당 노드의 데이터를 백업하고, 복원할 수 있는 방법이 있어야 한다. 때문에 각 Node에 Master-Slave 모델을 적용하여 보통은 Master가 Overlords와 통신을 하고 slave는 Master의 데이터를 주기적으로 백업하여 저장하도록 했다. Master가 정상작동 하지 않으면 Overlords는 slaves 중에 하나를 선택하여 Master로서의 역할을 수행하도록 한다.</p>
<p>{{thumbnail(그림2.png, size=600, title=Thumbnail)}}</p>
<p>추가적으로 각 노드의 변경 사항을 관리하고 노드들끼리 싱크를 맞추는 것은 직접 구현할 필요가 없고 ZooKeeper라는 좋은 관리 툴이 있기 때문에 분산 처리 환경 구축을 위해 해당 라이브러리를 사용하기로 했다. 또한 low level에서 네트워크로 통신하는 것도 직접 구현해야할 만한 이슈가 아니므로 ZeroMQ 라는 Message Queue를 사용하여 통신 구현을 쉽게 할 수 있도록 한다.</p>
<p>{{thumbnail(그림3.jpg, size=300, title=Thumbnail)}} {{thumbnail(그림4.jpg, size=300, title=Thumbnail)}}</p>
<h2>
<a aria-hidden="true" class="anchor" href="#6-project-architecture" id="user-content-6-project-architecture"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>6. Project Architecture</h2>
<ol>
	<li>Architecture Diagram<br/>
{{thumbnail(그림5.png, size=600, title=Thumbnail)}}<br/>
{{thumbnail(그림11.png, size=600, title=Architecture)}}</li>
	<li>Architecture Description</li>
</ol>
<ul>
	<li>
<strong>Application (DiMint Client)</strong><br/>
Overlord의 IP를 알고 있으며 해당 IP로 library 에서 제공하는 커맨드를 이용하여 DiMint Server System과 통신한다. DiMint Server에 관해서 IP 이외의 정보를 알 필요가 없기 때문에 사용하기 편리하다.</li>
</ul>
<ul>
	<li>
<strong>DiMint Server System</strong>
	<ul>
		<li>
<strong>Overlords</strong><br/>
세개의 서버로 구성되어 있으며 DiMint Client에서 보내는 모든 커맨드를 처리한다. 세개인 이유는 Overlords가 시스템의 핵심이기 때문에 서버가 세개인 경우 한 개의 서버가 정상 작동 하지 않아도 전체 시스템은 정상적으로 작동할 수 있기 때문이다. Overlords는 서비스를 요청한 Client의 정보를 가지고 consistent hashing을 하여 어떤 노드에 access하여 명령을 처리할 지 결정한다. 또한 항상 Node들의 부하를 체크하여 모니터링을 할 수 있도록 하며 관리자가 노드를 추가하거나 제거하는 경우에 필요한 처리들을 수행한다.</li>
		<li>
<strong>Nodes</strong><br/>
실제적인 데이터 저장소이다. 각 노드는 Overlords에 직접적으로 연결되어있고, 필요한 경우 Overlord에서 다른 노드의 정보를 받아와서 노드들끼리 서로 통신하여 data rebalancing을 수행한다.</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#7-implementation-spec" id="user-content-7-implementation-spec"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>7. Implementation Spec</h2>
<ol>
	<li>Input/Output Interface
	<ul>
		<li>Client<br/>
DiMinit Server의 remote storage를 이용하기 위한 인터페이스들을 제공한다.
		<ul>
			<li>get(key)<br/>
Server의 storage에서 input인 key에 mapping되어 저장된 value를 가져오기 위한 메서드이다. key는 내부에서 string type으로 변환되어 사용된다. 실행 결과값은 storage에 key에 mapping된 value가 존재하면 해당 value를 반환하며, 그렇지 않으면 Error message를 반환한다.</li>
			<li>set(key, value)<br/>
Server의 storage에 key와 value를 mapping하여 저장한다. key type이 string type으로 변환되어 사용되는 데에 반해, value type은 null, int, boolean, string, list, dictionary 등의 type을 지원한다. 정상적으로 저장이 되었을 경우 해당 value를 반환하며, 저장에 실패했을 경우 Error message를 반환한다.</li>
		</ul>
</li>
	</ul>
</li>
	<li>Inter Module Communication Interface</li>
</ol>
<p>{{thumbnail(그림12.png, size=650)}}<br/>
<img alt="" src="%EA%B7%B8%EB%A6%BC9.png"/></p>
<p>{{thumbnail(그림13.png, size=650)}}<br/>
<img alt="" src="%EA%B7%B8%EB%A6%BC8.png"/></p>
<p>{{thumbnail(그림14.png, size=650)}}</p>
<p>{{thumbnail(그림15.png, size=650)}}<br/>
<img alt="" src="%EA%B7%B8%EB%A6%BC10.png"/></p>
<ol>
	<li>Modules
	<ul>
		<li>Client<br/>
DiMinit Server의 remote storage를 이용하기 위한 인터페이스들을 제공한다.
		<ul>
			<li>OverlordConnector<br/>
Overlord와의 첫 연결에 관여하는 모듈이다.</li>
			<li>CommandCenter<br/>
Overlord에 각종 명령을 보내고, 그 결과를 받을 때 이용하는 모듈이다. get, set, monitor 명령이 이에 해당한다.</li>
		</ul>
</li>
		<li>Overlord<br/>
Client interface에서 제공하는 요청들을 처리하고, Node들을 관리한다.
		<ol>
			<li>Zookeeper<br/>
일반적으로 3대 이상을 사용하며 서버 수는 주로 홀수로 구성한다. 서버 간의 데이터 불일치가 발생하면 데이터 보정이 필요한데 이때 과반수의 룰을 적용하기 때문에 서버를 홀수로 구성하는 것이 데이터 적합성 측면에서 유리하다.<br/>
Zookeeper 서버는 Leader와 Follower로 구성되어 있다. 서버들끼리 자동으로 Leader를 선정하며 모든 데이터 저장을 주도한다.<br/>
<img alt="" src="%EA%B7%B8%EB%A6%BC6.png"/>
</li>
			<li>Consistent Hashing<br/>
Overlord에서 key를 sharding 할 때 사용한다.<br/>
Node가 동적으로 추가/제거되었을 때 모든 노드에 대해 key-value pair rebalancing 을 하지 않기 때문에 효율적이다.<br/>
<img alt="" src="%EA%B7%B8%EB%A6%BC7.png"/>
</li>
			<li>ClientConnector<br/>
Client의 접속을 받아들이는 모듈이다. 모든 Client의 명령은 여기서부터 시작되며, 명령은 NodeInfoManager에 전달된다. 또한 NodeInfoManager로부터 결과를 받아 Client에게 결과를 전송한다.</li>
			<li>NodeInfoManager<br/>
Overlord에 현재 연결되어 있는 Node의 정보를 저장하고 있는 모듈. HealthChecker 모듈을 통해 얻은 Node들의 현재 용량 상태, Node별 키 배분 현황, Node의 Master/Slave 상태, 각 키의 용량 등의 정보를 저장하고, 가지고 있는 정보를 제공하는 API를 가지고 있다. ClientConnector를 통해 get/set key 명령이 들어올 경우, 이 모듈을 통해 CommandSender에 명령을 전달한다.</li>
			<li>HealthChecker<br/>
각 node에 node의 상태를 요청하는 API를 전송하고, 결과를 받아 NodeInfoManager에 전송하는 모듈.</li>
			<li>CommandCenter<br/>
node에게 명령을 전송하고, 결과를 받는 모듈. 이 명령에서는 master node로 임명, rebalancing, get by key, set key value 등이 포함된다.</li>
			<li>NodeConnector<br/>
노드와의 연결을 관장하는 모듈. 새 node의 추가는 이 모듈을 통해 들어오며, NodeConnector는 NodeInfoManager에게 새 Node가 추가되었음을 알린다.</li>
			<li>Rebalancer<br/>
리밸런싱에 관여하는 모듈. 데이터 리밸런싱을 해야 하는 상황이 오면(노드의 추가, 제거) NodeInfoManager에서 현재 node들의 상황을 받아온 뒤, 어떤 node가 무슨 key를 어떤 node에 전송해야 할지 결정하고 CommandSender에 전송한다. 그 후 NodeInfoManager에 알려 Node의 key 배분 상태를 업데이트하게 한다.</li>
			<li>Master/Slave Manager<br/>
노드의 Master/Slave 관련 명령을 관장하는 모듈. Master Node 중 하나가 dead 상태가 되었을 경우, 해당 Master에 속해 있던 Slave Node 중 하나를 Master Node로 승격시키는 명령을 CommandCenter에 전송한다. 그리고 나서 NodeInfoManager에 해당 사실을 알려 Node 상태를 업데이트하게 한다.</li>
		</ol>
</li>
	</ul>
</li>
</ol>

	<ul>
		<li>Node<br/>
DiMint는 key-value 데이터를 메모리에 저장하여 관리한다.<br/>
단, 메모리는 Node가 다운되었을 때 데이터가 유실되는 문제가 있기 때문에 Master-Slave model로 구성하기로 했다.
		<ul>
			<li>OverlordConnector<br/>
overlord와의 접속에 관여하는 모듈. 처음 node process가 생성되면 overlord에 접속을 시도한다.</li>
			<li>CommandCenter<br/>
overlord의 CommandSender서 보내진 명령을 받고, 적절한 응답을 전달하는 모듈로, 여기서 어떤 명령인지 해석한 이후 Storage, Rebalancer, RoleManager, Recovery, HealthReporter 모듈 등에 명령이 전달된다.</li>
			<li>HealthReporter<br/>
Overlord의 HealthChecker가 node의 상태를 요청할 때, 자신의 상태를 전달하는 모듈. Stoarge 모듈로 가서 해당 node가 사용하는 memory 사용량, 각 key가 차지하는 용량 등을 overlord에 전송한다.</li>
			<li>Storage<br/>
실제로 Key-Value가 저장되어 있는 dictionary가 포함되어 있는 모듈. 단순히 key-value 저장/key에 대한 value값 리턴 이외에도 자신이 가지고 있는 key의 갯수 및 용량 정보를 가지고 있고 이를 리턴하는 API를 갖추고 있다.</li>
			<li>RoleManager<br/>
해당 node가 master node인 경우, Storage에 값이 저장되거나 변경될 때, Storage가 변경점을 RoleManager에게 전송하고 RoleManager는 자신의 slave node에 변경사항을 전송한다. 해당 node가 slave node인 경우, master node로부터 오는 log 정보를 받아 자신의 정보를 업데이트하기 위해 Stoarge 모듈에 해당 정보를 전달한다.</li>
			<li>Rebalancer<br/>
해당 node의 consistent hashing key가 변경되어 다른 node로 데이터를 옮겨야 할 경우, 다른 node의 주소와 옮겨야 할 키 목록을 overlord로부터 받아, 대상 node의 Rebalancer 모듈에 key-value dictionary를 전송하고, 원본 노드에서는 삭제한다. 반면, key-value dict를 받은 node 쪽에선 해당 dictionary를 Storage에 전달해서 key-value 정보를 업데이트한다</li>
		</ul>
</li>
	</ul><h2>
<a aria-hidden="true" class="anchor" href="#8-current-status" id="user-content-8-current-status"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>8. Current Status</h2>
<ul>
	<li>완료된 Issue
	<ul>
		<li>통신 프로토콜 정의</li>
		<li>client-to-overlord, overlord-to-node 간의 기본적인 통신 구현</li>
		<li>node의 read / write 구현</li>
	</ul>
</li>
</ul>
<ul>
	<li>진행중인 Issue
	<ul>
		<li>Overlord에 Zookeeper를 적용하고, Node 정보를 저장할 수 있도록 한다</li>
		<li>Node 추가 / 제거 / 장애 알림 시스템을 구현</li>
		<li>Consistent Hashing을 구현하고, 적절한 Hash function을 선택한다.</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#9-future-work" id="user-content-9-future-work"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>9. Future Work</h2>
<ul>
	<li>Overlord
	<ul>
		<li>Zookeeper가 적용된 overlord들을 연결한다.</li>
		<li>주기적으로 Node의 상태를 알아보기 위해 health_check를 호출하도록 한다.</li>
		<li>Consistent Hashing을 사용하여 sharding 방법으로 적용하도록 한다. (적당한 hash function 도 결정한다.)</li>
	</ul>
</li>
	<li>Node
	<ul>
		<li>Overlord에게 Node 추가 요청을 보낼 수 있도록 한다.</li>
		<li>Overlord에게 Node 제거 요청을 보낼 수 있도록 한다.</li>
		<li>키 별 데이터 용량을 Overlord에게 알릴 수 있도록 한다.</li>
		<li>각 Node에서 해당 프로세스의 메모리 점유율이 얼마나 되는지 알릴 수 있도록 한다.</li>
	</ul>
</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#10-division--assignment-of-work" id="user-content-10-division--assignment-of-work"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>10. Division &amp; Assignment of Work</h2>
<p>11개의 태스크를 Pair programming 으로 수행하도록 한다.</p>
<ul>
	<li>클라이언트 &amp; 프로토콜 – 김재찬, 오버로드 – 유재성, 노드 – 염지혜</li>
	<li>✓ 통신 프로토콜 정의 및 구현 : <strong>김재찬</strong> 유재성</li>
	<li>✓ 노드의 read/write 구현 : <strong>염지혜</strong> 유재성</li>
	<li>Zookeeper 노드 연결 구현 : <strong>유재성</strong> 염지혜</li>
	<li>노드 추가/제거 구현 : <strong>염지혜</strong> 유재성</li>
	<li>노드 장애 알림 구현 : <strong>김재찬</strong> 유재성</li>
	<li>Consistent hashing 구현 : <strong>유재성</strong> 김재찬</li>
	<li>데이터 rebalancing 구현 : <strong>유재성</strong> 염지혜</li>
	<li>Slave replicate 구현 : <strong>염지혜</strong> 김재찬</li>
	<li>Query off Loading 구현 : <strong>염지혜</strong> 유재성</li>
	<li>Master election 구현 : <strong>유재성</strong> 염지혜</li>
	<li>모니터링 기능 구현 : <strong>김재찬</strong> 염지혜</li>
	<li>테스트 클라이언트 구현 : <strong>김재찬</strong> 유재성</li>
</ul>
<h2>
<a aria-hidden="true" class="anchor" href="#11-schedule" id="user-content-11-schedule"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>11. Schedule</h2>
<p>{{thumbnail(schedule.png, size=600, title=Thumbnail)}}</p>
<h2>
<a aria-hidden="true" class="anchor" href="#12-demo" id="user-content-12-demo"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>12. Demo</h2>
<ol>
	<li>Overlord 구성
	<ul>
		<li>데모를 위해서 Overlord는 3개를 띄운다.</li>
		<li>데모를 위해 미리 띄워놓는 Overlord들은 302동 실습실의 리눅스가 설치되어 있는 PC에서 띄워놓는다. 이때 Overlord와 Node는 한 PC에서 동시에 실행될 수 있다.</li>
	</ul>
</li>
	<li>Node 구성
	<ul>
		<li>데모를 위해서 Master Node 3개, Slave Node 6개를 띄운다.</li>
		<li>데모를 위해 미리 띄워놓는 Node들은 302동 실습실의 리눅스가 설치되어 있는 PC에서 띄워놓는다.(총 9대 – Overlord 포함)</li>
		<li>스토리지 서비스를 중단하지 않고 Node의 개수를 늘리는 기능을 테스트
		<ul>
			<li>Master Node를 1개 더 추가한다.
			<ul>
				<li>모니터링 서비스로 각 Node들에 할당되는 key의 hash값 범위가 조정되는지 확인한다.</li>
				<li>이 때 추가되는 Node는 원격으로 동작시킬 수 있어야 하기 때문에 학부 서버(martini, midori)를 이용하거나, 넥슨에서 지원받은 리눅스 서버에서 추가한다.</li>
			</ul>
</li>
			<li>Slave Node를 1개 더 추가한다.</li>
		</ul>
</li>
		<li>총 4대의 Master Node가 있다면(원래 3대 + 테스트로 1대 추가) 테스트로 인해 추가된 Master Node 때문에 key-value를 다수 집어넣게 되면 이론적으로는 각 Node에 추가되는 key의 빈도에 불균형이 생긴다.
		<ul>
			<li>key의 빈도의 최대가 최소의 두배가 넘을 때 데이터 밸런싱이 일어나는 것을 확인한다.</li>
			<li>단, 이론적으로 빈도에 불균형이 새길 뿐, 데모때에는 key의 빈도의 최대가 최소의 두배가 넘는 경우가 생기지 않을 수 있다.</li>
		</ul>
</li>
		<li>추가되었던 Master Node를 강제로 죽이고 기존의 시스템이 정상동작하는지 확인한다.
		<ul>
			<li>총 11대의 Node로 실험을 하고 있었기 때문에(Master Node 4대 + Slave Node 7대) 10%의 이하의 Node가 서비스 불능일 때 모든 데이터에 문제 없이 접근할 수 있다는 평가 기준표를 만족할 수 있다.</li>
		</ul>
</li>
	</ul>
</li>
	<li>모니터링 서비스
	<ul>
		<li>데모 테스트 시간이 짧기 때문에 모니터링 서비스를 구축한다.</li>
		<li>python flask로 구현하며, Overlords에서 관리하는 Node들의 상태를 알 수 있도록 한다.
		<ul>
			<li>각 Node들의 생사 여부(죽었는지 살았는지)</li>
			<li>Master Node와 Slave Node 구분</li>
			<li>각 Node들에 할당되는 key의 hash값 범위</li>
			<li>각 Node들에 할당된 key의 빈도</li>
		</ul>
</li>
	</ul>
</li>
	<li>테스트 클라이언트
	<ul>
		<li>python flask로 구현하여 Client Library를 사용한다.</li>
		<li>평가기준표에 따르면 복잡한 형태의 데이터(set, list, dict 등)을 value로 저장할 수 있어야 한다.
		<ul>
			<li>value로 python set
			<ul>
				<li>eg. set(key, {1, 2, 3, 4, 5}), s = get(key)</li>
			</ul>
</li>
			<li>value로 python list
			<ul>
				<li>eg. set(key, [1, 2, 3, 4, 5]), l = get(key)</li>
			</ul>
</li>
			<li>value로 python dict
			<ul>
				<li>eg. set(key, {1 : “1”, 2 : “2”, 3 : “3”}), d = get(key)</li>
			</ul>
</li>
		</ul>
</li>
		<li>평가기준표에 따르면 다양한 type의 데이터를 key로 저장할 수 있다.
		<ul>
			<li>key로 int
			<ul>
				<li>eg. set(1, {1, 2, 3, 4, 5}), s = get(1)</li>
			</ul>
</li>
			<li>key로 float
			<ul>
				<li>eg. set(2.0, [1, 2, 3, 4, 5]), l = get(2.0)</li>
			</ul>
</li>
			<li>key로 string
			<ul>
				<li>eg. set(“3”, {1 : “1”, 2 : “2”, 3 : “3”}), d = get(“3”)</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
</li>
</ol>
<h2>
<a aria-hidden="true" class="anchor" href="#appendixdetailed-implementation-spec" id="user-content-appendixdetailed-implementation-spec"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>[Appendix]Detailed Implementation Spec</h2>

	<ul>
		<li>Client
		<ul>
			<li>OverloadConnector
			<ul>
				<li>connect(overlord_addr)<br/>
overlord에 접속한다. 이 때 overlord의 주소는 하나만 알고 있으면 된다.</li>
				<li>disconnect()<br/>
DiMint Server와의 접속을 종료한다.</li>
			</ul>
</li>
			<li>CommandCenter
			<ul>
				<li>get(key)<br/>
key에 해당하는 value 값을 요청한다.</li>
				<li>set(key, value)<br/>
주어진 key에 value를 mapping시켜 storage에 저장한다.</li>
				<li>get_node_status()<br/>
각 노드의 메모리 상태와 저장된 키의 갯수를 요청한다.</li>
			</ul>
</li>
		</ul>
</li>
		<li>Overlord
		<ul>
			<li>ClientConnector
			<ul>
				<li>get_overlords()<br/>
overlord들의 address 리스트를 반환한다.</li>
				<li>send_to_client(message)<br/>
send_to_message의 결과 메세지를 받아서 deserialize 한 후 write 인 경우에는 쓰인 key를 overlord가 관리하는 node key list에 추가한다. 각 키의 해싱값과 키에 따른 용량도 같이 dictionary로 저장한다.</li>
			</ul>
</li>
			<li>NodeInfoManager
			<ul>
				<li>get_message(message)<br/>
client에 bind된 message queue로부터 message를 받아 key값이 enable인지 확인하고, enable일 경우 key로 consistent hashing 하여 어떤 노드에 접근할 것인지 정한다. disable일 경우 에러 메시지를 날린다.</li>
				<li>get_all_node_states()<br/>
각 노드의 메모리 상태와 저장된 키의 갯수를 반환한다.</li>
				<li>set_enable(key_list, enable_type)<br/>
특정 key list에 대한 접근을 enable 해주는 메서드이다. 노드가 복구될 경우 해당 노드에 sharding되던 key list를 enable한다.<br/>
data rebalancing이 끝났을 경우, 모든 노드에 대해 write enabled, 이동한 key-value pair에 대해 read/write를 enable해야 한다.</li>
				<li>set_disable(key_list, disable_type)<br/>
특정 key list에 대한 접근을 disable 해주는 메서드이다. data rebalancing이 일어날 경우, 모든 노드에 대해 write를 disable, 다른 노드로 이동할 key-value pair에 대해 read/write disable한다. node가 offline 일 경우에도 해당 노드에 sharding 되던 key는 접근 불가능하도록 disable한다.</li>
			</ul>
</li>
			<li>HealthChecker
			<ul>
				<li>health_check()<br/>
모든 노드들의 상태를 체크한다. 노드에게 핑을 날리며, 노드들은 자신의 <span>CPU</span> 와 메모리의 상태를 반환해야 한다. timeout 시간 내에 반응하지 않는 노드는 offline으로 상태를 변경하고 이에 따른 처리를 시작한다.<br/>
offline인 노드가 master도 slave도 아니라면 우선 해당 노드가 sharding하던 key를 read/write disabled시키고, 복구되기를 기다린다. 설정된 복구 시간 내에 복구되지 않는다면, 해당 노드가 완전히 죽었다고 간주, consistent hashing을 변경한다. 이 때는 일부 노드의 consistent key coverage만 바뀔 뿐 key 이동은 없다.<br/>
offline인 노드가 master라면 일단 해당 node에 sharding되던 key는 read/write disabled시키고, 해당 노드의 slave 노드 중 하나를 master로 승격시킨다. master로 승격한 node는 자신의 정보를 master로 변경하고, 이웃(?) slave 노드는 master를 변경한다. 이 모든 일이 끝나면 disabled되었던 key를 다시 enabled시킨다. 이 때 consistent hashing은 변경하지 않는다. 만약 복구 시간 내에 offline이었던 노드가 복구된다 해도 이때는 새로운 master가 이미 선정된 뒤이므로, add_node를 통해 새로운 slave node를 추가하는 것과 같은 과정을 거치게 된다.<br/>
offline인 노드가 slave라면 특별히 처리할 일 없이, remove_node 를 하는 것과 같은 과정을 밟으면 된다.</li>
			</ul>
</li>
			<li>NodeConnector
			<ul>
				<li>add_node(host, port, type, master=None)<br/>
새로운 노드 후보가 노드로 추가해 달라는 요청을 보낼 때 호출된다. 추가에 성공한 경우 overlord가 관리하는 node에 관한 config를 바꾼다.<br/>
Master type 일 때 해당 노드의 hashing key 값을 정해주는데, 그 값은 다음과 같이 정해진다. key값 k는 기존에 있는 노드 중 가장 용량이 큰 노드를 대상으로, 노드가 가진 hashing key 값들을 k를 기준으로 두 그룹으로 나누었을 때 각 그룹의 키에 저장된 데이터의 용량 차이가 가장 작은 값이다. <br/>
Slave type 일 때 지정된 마스터 노드의 slave로 만들어주거나, 지정된 마스터가 없을 경우에는 slave를 더 받아야 하는 마스터 노드를 선택하여 해당 master 노드의 slave로 만들어준다.</li>
				<li>remove_node(host)<br/>
총 노드 수가 한 개일 경우 노드 삭제를 허용하지 않는다. <br/>
Master Node일 경우 해당 노드에 할당된 key list는 disabled 된다. add_node 때와 비슷하게 제거 대상 노드 N에 존재하는 key를 두 그룹으로 나눈다. 각 그룹을 이웃 master node로 옮기기 위해 Overlord가 move_key를 호출하여 복사를 시작한다. 복사가 정상적으로 끝나면 Overlord가 set_enable을 호출하여 disabled 되어 있었던 key list에 접근 가능하게 해준다.<br/>
Slave Node일 경우 해당 노드의 master node의 slave config에서 해당 노드를 삭제한 후 overlord config에서도 삭제한다.</li>
			</ul>
</li>
			<li>Rebalancer
			<ul>
				<li>move_key(key_list, dest_node)<br/>
해당 key를 가지고 있는 node에게, dest_node로 해당 key와 value 값을 옮기라고 요청한다.</li>
			</ul>
</li>
			<li>Master/Slave Manager
			<ul>
				<li>elect()<br/>
master node가 dead 상태가 되어 해당 노드의 slave node 중 하나를 새로운 master node로 elect해야 할 경우 호출된다. 새로운 master로 선출된 다른 slave가 있을 경우 해당 slave에게 master가 바뀌었음을 알리고 해당 slave를 config에 등록하고 elect가 완료됐음을 overlord에게 알린다.</li>
			</ul>
</li>
			<li>CommandCenter</li>
		</ul>
</li>
		<li>Node
		<ul>
			<li>OverlordConnector
			<ul>
				<li>connect_overlord(node_type)<br/>
overlord에 node로 추가해 달라고 요청한다.</li>
				<li>disconnect_overlord()<br/>
overlord에게 remove 요청을 보낸다.</li>
			</ul>
</li>
			<li>CommandCenter
			<ul>
				<li>get_message(message)<br/>
overlord로 부터 client가 요청한 메세지 내용을 받아서 command 타입에 따라 set 또는 get을 호출한다.</li>
				<li>sent_to_overlord(message)<br/>
overlord가 요청한 사항의 결과값을 overlord에게 전송한다.</li>
			</ul>
</li>
			<li>Storage
			<ul>
				<li>get(key)<br/>
node 가 관리하는 dictionary 에서 해당 key 값에 해당하는 value를 overlord에게 반환해준다. 만약 이 요청을 slave가 받았을 경우 적용되지 않은 operation log를 전부 적용 시킨 후에 key 값에 해당되는 value를 찾아서 반환한다.</li>
				<li>set(key. value)<br/>
node가 관리하는 dictionary에 요청한 key와 value를 삽입한다. 성공시에 value의 크기를 overlord에게 반환한다.</li>
			</ul>
</li>
			<li>HealthReporter</li>
			<li>RoleManager
			<ul>
				<li>log(operation, key, value=None)<br/>
master node에 set 메서드가 정상적으로 종료될 때 마다 해당 노드의 slave에 처리된 요청의 내용을 전송한다.</li>
				<li>dump()<br/>
자신의 slave에 자신이 가지고 있는 key-value storage 의 dump를 전송한다.</li>
				<li>set_master_node(addr)<br/>
자신이 slave 노드일 경우, master node를 설정한다.</li>
			</ul>
</li>
			<li>Rebalancer
			<ul>
				<li>move_data(key_list, dest_node_address)<br/>
key_list 에 있는 key와 그에 해당하는 value를 dest_node로 이동시킨다. 먼저 전송한 이후, 제대로 전송했다는 response가 오면 자신의 storage에서 해당 key-value pair들을 삭제한다.</li>
				<li>receive_data(key_value)<br/>
data rebalancing 이 있을 때, 혹은 node가 추가/삭제되었을 때 호출된다. 일반 set(key, value) 처럼 데이터를 자신의 storage에 넣은 뒤, source_node에 response를 보낸다.</li>
			</ul>
</li>
		</ul>
</li>
	</ul>
        </div>

    </div>]