[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h2>
<a aria-hidden="true" class="anchor" href="#2017-06-07-ci-infrastructure-partial-outages" id="user-content-2017-06-07-ci-infrastructure-partial-outages"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>2017-06-07 CI Infrastructure Partial Outages</h2>
<h3>
<a aria-hidden="true" class="anchor" href="#outage-details" id="user-content-outage-details"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Outage details</h3>
<p>Main outage discussed in <a href="https://github.com/nodejs/build/issues/749">https://github.com/nodejs/build/issues/749</a>, secondary outage in <a href="https://github.com/nodejs/build/issues/751">https://github.com/nodejs/build/issues/751</a>. In addition, we're continuing to experience problems with the Raspberry Pi 3's that <em>may</em> be related to the outage, that is being tracked in this issue: <a href="https://github.com/nodejs/build/issues/758">https://github.com/nodejs/build/issues/758</a>. This outage occurred just as @jasnell was preparing an 8.1.0 release as per <a href="https://github.com/nodejs/node/pull/13483">https://github.com/nodejs/node/pull/13483</a>.</p>
<p>The main outage occurred at @rvagg's hosted location where the bulk of the ARM resources for CI are held. This also happens to be the <em>only</em> current location of our macOS resources since we lost our Voxer machines and have not yet set up an alternative. The cause of the outage was a loss of power due to the local provider replacing some lines and a transformer nearby. The outage ran from approximately 11pm UTC to 4am UTC, at which point the resources were successfully reconnected. The key resources at this location required for releases are the macOS build machine, the Raspberry Pi 1 B+ machines that create our ARMv6 binaries and the AppliedMicro X-Gene ARM64 servers used to create our ARM64 / ARMv8 binaries.</p>
<p>The secondary outage occurred at <a href="https://osuosl.org/" rel="nofollow">OSUOSL</a> location that hosts all of our PPC hardware. There was a power outage here as well during approximately the same period. The key resources at this location required for resources are for building AIX binaries.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#impact" id="user-content-impact"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Impact</h3>
<p>The ARMv6 and ARM64 Linux, and the AIX binaries are considered of secondary importance while the macOS binaries are <em>critical</em> to releases. Therefore, the 8.1.0 release was postponed. This was unfortunate since there was an expectation set on social media and GitHub by @jasnell that the 8.1.0 would be out that day.</p>
<p>Test hardware that was unavailable during this time were:</p>
<ul>
<li>PPC infrastructure</li>
<li>All Raspberry Pi infrastructure: 1 B+, 2 and 3</li>
<li>AppliedMicro X-Gene ARM64 / ARMv8</li>
<li>macOS</li>
</ul>
<p>Scaleway ARMv7 and miniNodes ARM64 (Odroid C2) hardware were not impacted by this outage (i.e. not a complete ARM outage, and our ARMv7 binaries could still be produced).</p>
<h3>
<a aria-hidden="true" class="anchor" href="#resolution" id="user-content-resolution"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Resolution</h3>
<p>When power was restored to both locations, the CI nodes were reconnected to their Jenkins masters. Before reconnecting the Raspberry Pis, @rvagg cleaned out the workspaces (hosted on an SSD, mounted via NFS) and performed some updates on them, part of routine maintenance that is undertaken occasionally. @rvagg then proceeded to prepare the 8.1.0 release, taking over from @jasnell's mostly ready release proposal branch. The 8.1.0 release went ahead but the AIX and ARMv6 binaries were deferred until the next day as the AIX hardware recovery was delayed (@mhdawson took care of it when he came online for the day) and the ARMv6 binaries took extra time to compile, mainly due to having to create new workspaces and clone the repo from scratch. Additionally, there appears to exist a new (maybe?) problem on the network that primarily impacts the Raspberry Pi 3's but also seems to impact other Raspberry Pi's, although to a lesser extend. This is still being diagnosed and understood and is being documented in <a href="https://github.com/nodejs/node/pull/13483">https://github.com/nodejs/node/pull/13483</a>.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#weaknesses-exposed" id="user-content-weaknesses-exposed"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Weaknesses exposed</h3>
<p>The primary weakness exposed by this downtime was the fragility of our macOS resources. This is a well understood problem but the Build WG has been slow to come to a resolution. The Build WG has been keen to source donor hardware where possible and has preferred to exhaust its options before exploring paid options. Unfortunately this has left us in a position of only having a single full-time Mac Mini (plus an occasionally available one that hasn't been handling any meaningful load) running a two virtual machines, one for test and one for release. This creates a bottleneck as tests must be queued if run concurrently and slows down releases as release builds for macOS are divided into two separate jobs, one for .pkg and one for .tar (i.e. they must run in serial whereas in the past they have run in parallel on different machines). In addition, this single Mac Mini is located on @rvagg's network which has been one of our least reliable networksâ€”this doesn't pair well with the criticality of macOS in our priorities for both test and build.</p>
<p>AIX / PPC, ARMv6, ARMv7 and ARM64 were all also impacted (partially or completely), however due to their low download numbers (see <a href="https://nodejs.org/metrics/" rel="nofollow">metrics</a>), we do not consider these critical in the same way as the macOS resources. It has historically been considered reasonable to go ahead with release builds without completion of these builds, promoting them after the fact (the release blog post script inserts a <em>"Coming soon"</em> where they are missing). It may also be considered reasonable to proceed with merging pull requests if tests cannot be run on these platforms if they successfully run on the remaining platforms. ARM64 and ARMv7 have hardware located elsewhere and tests could still be run on both during the downtime. Release builds for ARMv7 were available but ARM64 and ARMv6 were not.</p>
<h3>
<a aria-hidden="true" class="anchor" href="#action-items-post-outage" id="user-content-action-items-post-outage"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Action items post-outage</h3>
<ul>
<li>The Build WG has since secured a partnership with MacStadium to provide considerable cloud resources to the project, more details can be found in <a href="https://github.com/nodejs/build/issues/756">https://github.com/nodejs/build/issues/756</a>, this is a work in progress but it will allow us to either retire the current macOS hardware or use it for additional redundancy. The Build WG continues to reach out to other potential partners to increase our macOS resiliency.</li>
<li>packet.net recently reached a partnership with the project to provide access to modern server-class bare-metal ARM64 hardware. Since the outage, the Build WG has connected new instances for both test and release, see <a href="https://github.com/nodejs/build/issues/755">https://github.com/nodejs/build/issues/755</a> for some context. These new options already make our older ARM64 infrastructure redundant and they are likely to be decomissioned. In addition, Scaleway has released some new ARM64 options and we should be able to expand our partnership there to provide additional redundancy.</li>
<li>NFS / network speed problems are still being explored, see <a href="https://github.com/nodejs/build/issues/758">https://github.com/nodejs/build/issues/758</a>
</li>
<li>The Build WG has had brief discussions about the possibility of either splitting the ARM cluster up into multiple locations (this involves some unfortunate maintenance overhead that we may be best to avoid), <em>or</em> connecting some backup Raspberry Pi hardware to be used exclusively for release builds, as a fail-over.</li>
<li>The Build WG has not discussed redundancy for PPC / AIX hardware, but this is considered a low priority so has not been a focus.</li>
</ul>
<p>In summary, the Build WG has already improved resilience of some of the hardware impacted by this outage. Within a matter of weeks we should be at a state where similar outages will not be reasonable cause to hold up releases.</p>
<p><strong>Please continue any discussion in <a href="https://github.com/nodejs/build/issues/749">https://github.com/nodejs/build/issues/749</a></strong></p>

        </div>

    </div>]